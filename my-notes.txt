- Training: OOM
- empty_cache_freq 128 -> 32: still OOM, around the same step as above, seems not helpful; change back
- batch_size 8 -> 4: is able to train. Using this for now

7.5s/it, 33699it/epoch, about 70h/epoch, max 10 epochs, about 30 days

batch_size = 6: is runnable, still ~70h/epoch; GPU memory ~7500/12000MB (might be inaccurate)
batch_size = 8, fp16: is runnable(!), ~60h/epoch, GPU memory ~6600/12000MB (might be inaccurate)

Preprocessing issue:
[UNK] exists in preprocessed files (generated by common_crawl.py and extract_wiki_data.py). It would be tokenized after acoustic confusion injection, and cause subword mismatch, so these samples would be discarded.
Estimated using epoch_X.shard0.sample.json, it takes up less than 0.5% of the data. Currently ignoring this issue (just let the program discarding these samples)