{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import ujson\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/CMUdict/cmudict-0.7b.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133854"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry_lines = []\n",
    "\n",
    "with open(cmudict_path, 'r', encoding='latin-1') as f:\n",
    "    for l in f:\n",
    "        if len(l.strip()) > 0 and (not l.startswith(';;;')):\n",
    "            entry_lines.append(l.strip())\n",
    "\n",
    "len(entry_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"IN-QUOTES  IH1 N K W OW1 T S',\n",
       " 'BELDOCK  B EH1 L D AA2 K',\n",
       " 'CHANNELED  CH AE1 N AH0 L D',\n",
       " 'DEMILLE(1)  D IH0 M IH1 L',\n",
       " 'EXTORTIONISTS  EH0 K S T AO1 R SH AH0 N IH0 S T S',\n",
       " 'GRIPPED  G R IH1 P T',\n",
       " 'INTERSPERSES  IH2 N T ER0 S P ER1 S AH0 Z',\n",
       " 'LIMESTONE  L AY1 M S T OW2 N',\n",
       " 'MONROEVILLE  M AA0 N R OW1 V IH2 L',\n",
       " 'PENDERGAST  P EH1 N D ER0 G AE2 S T',\n",
       " 'REPEATS(1)  R IY0 P IY1 T S',\n",
       " 'SIELAFF  S IY0 L AE1 F',\n",
       " 'THIBEDEAU  TH IH1 B IH0 D OW0',\n",
       " 'WHITEAKER(1)  HH W IH1 T AH0 K ER0']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry_lines[5::10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"IN-QUOTES None ('IH', 'N', 'K', 'W', 'OW', 'T', 'S')\n",
      "BELDOCK None ('B', 'EH', 'L', 'D', 'AA', 'K')\n",
      "CHANNELED None ('CH', 'AE', 'N', 'AH', 'L', 'D')\n",
      "DEMILLE 1 ('D', 'IH', 'M', 'IH', 'L')\n",
      "EXTORTIONISTS None ('EH', 'K', 'S', 'T', 'AO', 'R', 'SH', 'AH', 'N', 'IH', 'S', 'T', 'S')\n",
      "GRIPPED None ('G', 'R', 'IH', 'P', 'T')\n",
      "INTERSPERSES None ('IH', 'N', 'T', 'ER', 'S', 'P', 'ER', 'S', 'AH', 'Z')\n",
      "LIMESTONE None ('L', 'AY', 'M', 'S', 'T', 'OW', 'N')\n",
      "MONROEVILLE None ('M', 'AA', 'N', 'R', 'OW', 'V', 'IH', 'L')\n",
      "PENDERGAST None ('P', 'EH', 'N', 'D', 'ER', 'G', 'AE', 'S', 'T')\n",
      "REPEATS 1 ('R', 'IY', 'P', 'IY', 'T', 'S')\n",
      "SIELAFF None ('S', 'IY', 'L', 'AE', 'F')\n",
      "THIBEDEAU None ('TH', 'IH', 'B', 'IH', 'D', 'OW')\n",
      "WHITEAKER 1 ('HH', 'W', 'IH', 'T', 'AH', 'K', 'ER')\n"
     ]
    }
   ],
   "source": [
    "def strip_stress(phone: str) -> str:\n",
    "    _m = re.match(r'(.*)\\d+$', phone)\n",
    "    if _m is not None:\n",
    "        phone = _m.group(1)\n",
    "    return phone\n",
    "\n",
    "# test \n",
    "for l in entry_lines[5::10000]:\n",
    "    _word, _pron = l.split('  ')\n",
    "    \n",
    "    if _word.endswith(')'):\n",
    "        # variant\n",
    "        _m = re.match(r'(.*)\\((.*)\\)$', _word)\n",
    "        _word = _m.group(1)\n",
    "        _variant = _m.group(2)\n",
    "    else:\n",
    "        # no variant\n",
    "        _variant = None\n",
    "    \n",
    "    _phones = tuple([strip_stress(_phone) for _phone in _pron.split(' ')])\n",
    "    \n",
    "    print(_word, _variant, _phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fd9c9a43504ab0ae8233d1a562afb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=133854), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(125074, 113745)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2pron = defaultdict(set) # all possible prons, Dict[str, Set[Tuple[str]]]\n",
    "pron2word = defaultdict(set) # all possible words, Dict[Tuple[str], Set[str]]\n",
    "\n",
    "for l in tqdm(entry_lines):\n",
    "    _word, _pron = l.split('  ')\n",
    "    \n",
    "    if _word.endswith(')'):\n",
    "        # variant\n",
    "        _m = re.match(r'(.*)\\((.*)\\)$', _word)\n",
    "        _word = _m.group(1)\n",
    "        _variant = _m.group(2)\n",
    "    else:\n",
    "        # no variant\n",
    "        _variant = None\n",
    "    \n",
    "    _phones = tuple([strip_stress(_phone) for _phone in _pron.split(' ')])\n",
    "    \n",
    "    word2pron[_word].add(_phones)\n",
    "    pron2word[_phones].add(_word)\n",
    "\n",
    "len(word2pron), len(pron2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125074, 113745)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word = list(word2pron.keys())\n",
    "word2idx = {w : idx for idx, w in enumerate(idx2word)}\n",
    "idx2pron = list(pron2word.keys())\n",
    "pron2idx = {p : idx for idx, p in enumerate(idx2pron)}\n",
    "len(word2idx), len(pron2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('R', 'AH', 'K', 'AO', 'R', 'D'),\n",
       " ('R', 'EH', 'K', 'ER', 'D'),\n",
       " ('R', 'IH', 'K', 'AO', 'R', 'D')}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2pron['RECORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91994, 'RECORD')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['RECORD'], idx2word[91994]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83079, ('R', 'EH', 'D'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pron2idx[('R', 'EH', 'D')], idx2pron[83079]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suffixes\n",
    "suffixes = [\n",
    "    ['S'],\n",
    "    ['IY NG'],\n",
    "    ['IH NG'],\n",
    "    ['D']\n",
    "]\n",
    "\n",
    "# Similar phone clusters \n",
    "clusters = [\n",
    "    ['Z', 'S'],\n",
    "    ['AA', 'AO', 'EY', 'UH'],\n",
    "    ['AXR', 'AX'],\n",
    "    ['P', 'B', 'F'],\n",
    "    ['DH', 'CH', 'ZH', 'T', 'SH'],\n",
    "    ['IY', 'AY', 'OW'],\n",
    "    ['EH', 'AH', 'IH', 'AW', 'ER', 'UW']\n",
    "]\n",
    "\n",
    "phone2cluster = defaultdict(list) # Dict[str, List(str)]\n",
    "\n",
    "for c in clusters:\n",
    "    for p in c:\n",
    "        phone2cluster[p] = c\n",
    "\n",
    "len(phone2cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IY', 'AY', 'OW']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone2cluster['IY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be33883d6b6144778bb7b7be20eaafbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove consonant: QUOTE('K', 'W', 'OW', 'T') -> COAT('K', 'OW', 'T')\n",
      "Remove consonant: QUOTE('K', 'W', 'OW', 'T') -> QUO('K', 'W', 'OW')\n",
      "Add suffix: QUOTE('K', 'W', 'OW', 'T') -> QUOTES('K', 'W', 'OW', 'T', 'S')\n",
      "Substitute phone: QUOTE('K', 'W', 'OW', 'T') -> QUIETT('K', 'W', 'IY', 'T')\n",
      "Substitute phone: QUOTE('K', 'W', 'OW', 'T') -> QUITE('K', 'W', 'AY', 'T')\n",
      "Remove consonant: \"UNQUOTE('AH', 'N', 'K', 'W', 'OW', 'T') -> UNCOAT('AH', 'N', 'K', 'OW', 'T')\n",
      "Remove consonant: PERCENT('P', 'ER', 'S', 'EH', 'N', 'T') -> PERSET('P', 'ER', 'S', 'EH', 'T')\n",
      "Add suffix: PERCENT('P', 'ER', 'S', 'EH', 'N', 'T') -> PERCENTS('P', 'ER', 'S', 'EH', 'N', 'T', 'S')\n",
      "Substitute phone: PERCENT('P', 'ER', 'S', 'EH', 'N', 'T') -> PRESENT('P', 'ER', 'Z', 'EH', 'N', 'T')\n",
      "Remove consonant: 'ALLO('AA', 'L', 'OW') -> LOE('L', 'OW')\n",
      "Substitute phone: 'ALLO('AA', 'L', 'OW') -> OLLY('AA', 'L', 'IY')\n",
      "Add suffix: BOUT('B', 'AW', 'T') -> BOUTS('B', 'AW', 'T', 'S')\n",
      "Substitute phone: BOUT('B', 'AW', 'T') -> POUT('P', 'AW', 'T')\n",
      "Add suffix: KAY('K', 'EY') -> CAYCE('K', 'EY', 'S')\n",
      "Add suffix: KAY('K', 'EY') -> KADE('K', 'EY', 'D')\n",
      "Double vowel: A('EY',) -> AA('EY', 'EY')\n",
      "Double vowel: ACQUIRE('AH', 'K', 'W', 'AY', 'ER') -> ACQUIRER('AH', 'K', 'W', 'AY', 'ER', 'ER')\n",
      "Double vowel: ACQUIRES('AH', 'K', 'W', 'AY', 'ER', 'Z') -> ACQUIRERS('AH', 'K', 'W', 'AY', 'ER', 'ER', 'Z')\n",
      "Double vowel: ADVENTURE('AE', 'D', 'V', 'EH', 'N', 'CH', 'ER') -> ADVENTURER('AE', 'D', 'V', 'EH', 'N', 'CH', 'ER', 'ER')\n",
      "Double vowel: ADVENTURE('AH', 'D', 'V', 'EH', 'N', 'CH', 'ER') -> ADVENTURER('AH', 'D', 'V', 'EH', 'N', 'CH', 'ER', 'ER')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1-step confusion \n",
    "pron_confusions = [set() for _ in idx2pron] # confusable pron ids, List[int]; [i] has j: i can be replaced by j  \n",
    "\n",
    "remove_consonant_cnt = 0\n",
    "double_vowel_cnt = 0\n",
    "add_suffix_cnt = 0\n",
    "substitute_cnt = 0\n",
    "\n",
    "for _idx, _pron in tqdm(enumerate(idx2pron), total=len(idx2pron)):\n",
    "    _phones = list(_pron)\n",
    "    \n",
    "    # remove a consonant (outgoing edge only)\n",
    "    for j in range(len(_phones)):\n",
    "        _confs_pron = tuple(_phones[:j] + _phones[j+1:])\n",
    "        try:\n",
    "            _confs_idx = pron2idx[_confs_pron]\n",
    "            pron_confusions[_idx].add(_confs_idx)\n",
    "            remove_consonant_cnt += 1\n",
    "            \n",
    "            if remove_consonant_cnt <= 5:\n",
    "                _src_pron = _pron\n",
    "                _src_w = next(iter(pron2word[_src_pron]))\n",
    "                _tgt_pron = _confs_pron\n",
    "                _tgt_w = next(iter(pron2word[_tgt_pron]))\n",
    "                print(f'Remove consonant: {_src_w}{_src_pron} -> {_tgt_w}{_tgt_pron}')\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # remove a doubled vowel (incoming edge only, for doubling vowel)\n",
    "    for j in range(1, len(_phones)):\n",
    "        if _phones[j] != _phones[j-1] or (not _phones[j][0] in 'AEIOU'):\n",
    "            continue\n",
    "            \n",
    "        _confs_pron = tuple(_phones[:j] + _phones[j+1:])\n",
    "        try:\n",
    "            _confs_idx = pron2idx[_confs_pron]\n",
    "            pron_confusions[_confs_idx].add(_idx)\n",
    "            double_vowel_cnt += 1\n",
    "            \n",
    "            if double_vowel_cnt <= 5:\n",
    "                _src_pron = _confs_pron\n",
    "                _src_w = next(iter(pron2word[_src_pron]))\n",
    "                _tgt_pron = _pron\n",
    "                _tgt_w = next(iter(pron2word[_tgt_pron]))\n",
    "                print(f'Double vowel: {_src_w}{_src_pron} -> {_tgt_w}{_tgt_pron}')\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # add a suffix (outgoing edge only)\n",
    "    for _suffix in suffixes:\n",
    "        _confs_pron = tuple(_phones + _suffix)\n",
    "        try:\n",
    "            _confs_idx = pron2idx[_confs_pron]\n",
    "            pron_confusions[_idx].add(_confs_idx)\n",
    "            add_suffix_cnt += 1\n",
    "            \n",
    "            if add_suffix_cnt <= 5:\n",
    "                _src_pron = _pron\n",
    "                _src_w = next(iter(pron2word[_src_pron]))\n",
    "                _tgt_pron = _confs_pron\n",
    "                _tgt_w = next(iter(pron2word[_tgt_pron]))\n",
    "                print(f'Add suffix: {_src_w}{_src_pron} -> {_tgt_w}{_tgt_pron}')\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # substitute a phone (outgoing edge only; it's bidirectional in essense)\n",
    "    for j in range(len(_phones)):\n",
    "        _ph = _phones[j]\n",
    "        _ph_cluster = phone2cluster[_ph]\n",
    "        if len(_ph_cluster) <= 1:\n",
    "            continue\n",
    "        \n",
    "        # has a cluster, do replace\n",
    "        for _confs_ph in _ph_cluster:\n",
    "            if _confs_ph == _ph:\n",
    "                continue\n",
    "            \n",
    "            _confs_pron = tuple(_phones[:j] + [_confs_ph] + _phones[j+1:])\n",
    "            try:\n",
    "                _confs_idx = pron2idx[_confs_pron]\n",
    "                pron_confusions[_idx].add(_confs_idx)\n",
    "                substitute_cnt += 1\n",
    "\n",
    "                if substitute_cnt <= 5:\n",
    "                    _src_pron = _pron\n",
    "                    _src_w = next(iter(pron2word[_src_pron]))\n",
    "                    _tgt_pron = _confs_pron\n",
    "                    _tgt_w = next(iter(pron2word[_tgt_pron]))\n",
    "                    print(f'Substitute phone: {_src_w}{_src_pron} -> {_tgt_w}{_tgt_pron}')\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86039, 86, 7700, 51210)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_consonant_cnt, double_vowel_cnt, add_suffix_cnt, substitute_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 45130),\n",
       " (1, 33351),\n",
       " (2, 15480),\n",
       " (3, 8802),\n",
       " (4, 5215),\n",
       " (5, 3086),\n",
       " (6, 1489),\n",
       " (7, 759),\n",
       " (8, 292),\n",
       " (9, 109),\n",
       " (10, 18),\n",
       " (11, 11),\n",
       " (12, 3)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter([len(x) for x in pron_confusions]).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word confusion \n",
    "word_confusions = defaultdict(set) # confusable words (not ids!), Dict[str, Set[str]]\n",
    "\n",
    "for _word in idx2word:\n",
    "    for _pron in word2pron[_word]:\n",
    "        _pron_idx = pron2idx[_pron]\n",
    "        _confs_pron_ids = pron_confusions[_pron_idx]\n",
    "        for _confs_pron_idx in _confs_pron_ids:\n",
    "            _confs_pron = idx2pron[_confs_pron_idx]\n",
    "            _confs_words = pron2word[_confs_pron]\n",
    "            word_confusions[_word].update(_confs_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 25),\n",
       " (2, 223),\n",
       " (3, 1513),\n",
       " (4, 6440),\n",
       " (5, 13435),\n",
       " (6, 17669),\n",
       " (7, 15616),\n",
       " (8, 10357),\n",
       " (9, 6634),\n",
       " (10, 4123),\n",
       " (11, 2422),\n",
       " (12, 1307),\n",
       " (13, 695),\n",
       " (14, 322),\n",
       " (15, 152),\n",
       " (16, 70),\n",
       " (17, 23),\n",
       " (18, 12),\n",
       " (19, 3),\n",
       " (20, 3)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter([len(x) for x in word_confusions]).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('R', 'EH', 'D')}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2pron['RED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RUUD', {('R', 'UW', 'D')}),\n",
       " ('ROOD', {('R', 'UW', 'D')}),\n",
       " ('RUDE', {('R', 'UW', 'D')}),\n",
       " ('ED', {('EH', 'D')}),\n",
       " ('RHUDE', {('R', 'UW', 'D')}),\n",
       " ('REH', {('R', 'EH')}),\n",
       " ('RUDD', {('R', 'AH', 'D')}),\n",
       " ('ROODE', {('R', 'UW', 'D')}),\n",
       " ('RUD', {('R', 'AH', 'D')}),\n",
       " ('RID', {('R', 'IH', 'D')})]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w, word2pron[w]) for w in word_confusions['RED']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence confusion \n",
    "\n",
    "def SentenceAcousticConfusion(sentence: List[str], p: float) -> List[str]:\n",
    "    assert 0 <= p <= 1\n",
    "    \n",
    "    sen_len = len(sentence)\n",
    "    confs_cnt = int(p * sen_len)\n",
    "    \n",
    "    confusable_positions = []\n",
    "    for pos in range(sen_len):\n",
    "        word = sentence[pos].upper()\n",
    "        if len(word_confusions[word]) > 0:\n",
    "            confusable_positions.append(pos)\n",
    "    \n",
    "    if len(confusable_positions) <= confs_cnt:\n",
    "        # not enough positions for confusion\n",
    "        confs_positions = confusable_positions\n",
    "    else:\n",
    "        confs_positions = random.sample(confusable_positions, k=confs_cnt)\n",
    "        \n",
    "    confs_sentence = list(sentence)\n",
    "    for pos in confs_positions:\n",
    "        word = sentence[pos].upper()\n",
    "        confs_word = random.choice(list(word_confusions[word])).lower()\n",
    "        confs_sentence[pos] = confs_word\n",
    "    \n",
    "    return confs_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'are',\n",
       " 'though',\n",
       " 'names',\n",
       " 'of',\n",
       " 'auld',\n",
       " 'European',\n",
       " 'countries',\n",
       " 'with',\n",
       " 'at',\n",
       " 'least',\n",
       " 'three',\n",
       " 'manufacturers',\n",
       " '?']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\n",
    "    \"What\",\n",
    "    \"are\",\n",
    "    \"the\",\n",
    "    \"names\",\n",
    "    \"of\",\n",
    "    \"all\",\n",
    "    \"European\",\n",
    "    \"countries\",\n",
    "    \"with\",\n",
    "    \"at\",\n",
    "    \"least\",\n",
    "    \"three\",\n",
    "    \"manufacturers\",\n",
    "    \"?\"\n",
    "]\n",
    "\n",
    "SentenceAcousticConfusion(sentence, p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uuid': 'common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042992201.62/warc/CC-MAIN-20150728002312-00130-ip-10-236-191-2.ec2.internal.warc.gz_567892730_567906623',\n",
       " 'table': {'caption': '',\n",
       "  'header': [{'name': 'Class',\n",
       "    'name_tokens': None,\n",
       "    'type': 'text',\n",
       "    'sample_value': {'value': '0', 'tokens': ['0'], 'ner_tags': ['']},\n",
       "    'sample_value_tokens': None,\n",
       "    'is_primary_key': False,\n",
       "    'foreign_key': None},\n",
       "   {'name': 'Interpretation',\n",
       "    'name_tokens': None,\n",
       "    'type': 'text',\n",
       "    'sample_value': {'value': 'Negative',\n",
       "     'tokens': ['Negative'],\n",
       "     'ner_tags': ['']},\n",
       "    'sample_value_tokens': None,\n",
       "    'is_primary_key': False,\n",
       "    'foreign_key': None}],\n",
       "  'data': [['0', 'Negative'],\n",
       "   ['1', 'Equivocal'],\n",
       "   ['2', 'Positive'],\n",
       "   ['3', 'Positive'],\n",
       "   ['4', 'Strongly positive'],\n",
       "   ['5', 'Strongly positive'],\n",
       "   ['6', 'Strongly positive']]},\n",
       " 'context_before': ['Reference Values Describes reference intervals and additional information for interpretation of test results.',\n",
       "  'May include intervals based on age and sex when appropriate.',\n",
       "  'Intervals are Mayo-derived, unless otherwise designated.',\n",
       "  'If an interpretive report is provided, the reference value field will state this.',\n",
       "  'Some individuals with clinically insignificant sensitivity to allergens may have measurable levels of IgE antibodies in serum, and results must be interpreted in the clinical context.',\n",
       "  'Testing for IgE antibodies is not useful in patients previously treated with immunotherapy to determine if residual clinical sensitivity exists, or in patients in whom the medical management does not depend upon identification of allergen specificity.',\n",
       "  'Cautions Discusses conditions that may cause diagnostic confusion, including improper specimen collection and handling, inappropriate test selection, and interfering substances The level of IgE antibodies in serum varies directly with the concentration of IgE antibodies expressed as a class score or kU/L. Detection of IgE antibodies in serum (Class 1 or greater) indicates an increased likelihood of allergic disease as opposed to other etiologies and defines the allergens that may be responsible for eliciting signs and symptoms.',\n",
       "  'Interpretation Provides information to assist in interpretation of the test results Testing also'],\n",
       " 'context_after': ['Reference values apply to all ages.',\n",
       "  'Clinical References Provides recommendations for further in-depth reading of a clinical nature Homburger HA: Chapter 53: Allergic diseases.',\n",
       "  'In Clinical Diagnosis and Management by Laboratory Methods.',\n",
       "  'Edited by RA McPherson, MR Pincus.',\n",
       "  'All Rights Reserved.',\n",
       "  'Key Registration Required Transactional Access Commercial Access Secondary Access Primary Access External link PDF document Excel document Word document']}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply on tables.jsonl \n",
    "tabert_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/TaBERT_datasets/tables_sample.jsonl'\n",
    "\n",
    "with open(tabert_dataset_path, 'r') as f:\n",
    "    l = f.readline()\n",
    "\n",
    "# print(ujson.dumps(ujson.loads(l), indent=4))\n",
    "d = ujson.loads(l)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uuid': 'common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042992201.62/warc/CC-MAIN-20150728002312-00130-ip-10-236-191-2.ec2.internal.warc.gz_567892730_567906623',\n",
       " 'table': {'caption': '',\n",
       "  'header': [{'name': 'Class',\n",
       "    'name_tokens': None,\n",
       "    'type': 'text',\n",
       "    'sample_value': {'value': '0', 'tokens': ['0'], 'ner_tags': ['']},\n",
       "    'sample_value_tokens': None,\n",
       "    'is_primary_key': False,\n",
       "    'foreign_key': None},\n",
       "   {'name': 'Interpretation',\n",
       "    'name_tokens': None,\n",
       "    'type': 'text',\n",
       "    'sample_value': {'value': 'Negative',\n",
       "     'tokens': ['Negative'],\n",
       "     'ner_tags': ['']},\n",
       "    'sample_value_tokens': None,\n",
       "    'is_primary_key': False,\n",
       "    'foreign_key': None}],\n",
       "  'data': [['0', 'Negative'],\n",
       "   ['1', 'Equivocal'],\n",
       "   ['2', 'Positive'],\n",
       "   ['3', 'Positive'],\n",
       "   ['4', 'Strongly positive'],\n",
       "   ['5', 'Strongly positive'],\n",
       "   ['6', 'Strongly positive']]},\n",
       " 'context_before': ['Reference Values Describes reference intervals uhde additional information for interpretation irv test results .',\n",
       "  'May include intervals based on age ahn sex when appropriate .',\n",
       "  'Intervals ayr Mayo-derived , unless otherwise designated .',\n",
       "  'If an interpretive report is provided , the reference value field well state this .',\n",
       "  \"Some individuals' with clinical insignificant sensitivity to allergens may have measurable levels of ai antibodies in serum , and results must be interpreted n. the clinical context .\",\n",
       "  \"Testing for IgE antibody is knott's useful in patients previously treated with immunotherapy to determine if residual clinical sensitivity exists , or in patience in whom shih medical management does not depend pon identification of allergen specificity .\",\n",
       "  \"Cautions Discusses condition that may cause diagnostic confusion , including improper specimen collection 'n handling , inappropriate test selection , ende interfering substances a lovell of IgE antibodies in serum varies directly wich the concentration of IgE antibodies expressed 's a class saur or kU/L . Detection of IgE antibodies in serum ( Class 1 or grayer ) indicates an increased likelihood of allergic disease as appeased to other etiologies and defines the allergens that made be responsible for eliciting signs and symptoms .\",\n",
       "  'Interpretation provide information to assist in interpretation of the test results Testing also'],\n",
       " 'context_after': ['Reference values apply to aw ages .',\n",
       "  'Clinical References Provides recommendations er further in-depth reading of ou clinical nature Homburger HA : apter 53 : Allergic diseases .',\n",
       "  'In Clinical Diagnosis and Management i. Laboratory Methods .',\n",
       "  'Edited by RA McPherson , muster Pincus .',\n",
       "  'All Rights Reserved .',\n",
       "  \"Key Registration Required Transactional Access Commercial axis Secondary Access Primary Access External link's PDF document Excel document Word document\"]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.15\n",
    "\n",
    "confs_d = d.copy()\n",
    "for idx, sen in enumerate(d['context_before']):\n",
    "    sen_tokens = word_tokenize(sen)\n",
    "    sen_tokens_confused = SentenceAcousticConfusion(sen_tokens, p)\n",
    "    sen_confused = ' '.join(sen_tokens_confused)\n",
    "    confs_d['context_before'][idx] = sen_confused\n",
    "for idx, sen in enumerate(d['context_after']):\n",
    "    sen_tokens = word_tokenize(sen)\n",
    "    sen_tokens_confused = SentenceAcousticConfusion(sen_tokens, p)\n",
    "    sen_confused = ' '.join(sen_tokens_confused)\n",
    "    confs_d['context_after'][idx] = sen_confused\n",
    "confs_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
