{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import ujson\n",
    "import numpy as np\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from table_bert.utils import BertTokenizer\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/CMUdict/cmudict-0.7b.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133854"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry_lines = []\n",
    "\n",
    "with open(cmudict_path, 'r', encoding='latin-1') as f:\n",
    "    for l in f:\n",
    "        if len(l.strip()) > 0 and (not l.startswith(';;;')):\n",
    "            entry_lines.append(l.strip())\n",
    "\n",
    "len(entry_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"IN-QUOTES  IH1 N K W OW1 T S',\n",
       " 'BELDOCK  B EH1 L D AA2 K',\n",
       " 'CHANNELED  CH AE1 N AH0 L D',\n",
       " 'DEMILLE(1)  D IH0 M IH1 L',\n",
       " 'EXTORTIONISTS  EH0 K S T AO1 R SH AH0 N IH0 S T S',\n",
       " 'GRIPPED  G R IH1 P T',\n",
       " 'INTERSPERSES  IH2 N T ER0 S P ER1 S AH0 Z',\n",
       " 'LIMESTONE  L AY1 M S T OW2 N',\n",
       " 'MONROEVILLE  M AA0 N R OW1 V IH2 L',\n",
       " 'PENDERGAST  P EH1 N D ER0 G AE2 S T',\n",
       " 'REPEATS(1)  R IY0 P IY1 T S',\n",
       " 'SIELAFF  S IY0 L AE1 F',\n",
       " 'THIBEDEAU  TH IH1 B IH0 D OW0',\n",
       " 'WHITEAKER(1)  HH W IH1 T AH0 K ER0']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry_lines[5::10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"IN-QUOTES None ('IH', 'N', 'K', 'W', 'OW', 'T', 'S')\n",
      "BELDOCK None ('B', 'EH', 'L', 'D', 'AA', 'K')\n",
      "CHANNELED None ('CH', 'AE', 'N', 'AH', 'L', 'D')\n",
      "DEMILLE 1 ('D', 'IH', 'M', 'IH', 'L')\n",
      "EXTORTIONISTS None ('EH', 'K', 'S', 'T', 'AO', 'R', 'SH', 'AH', 'N', 'IH', 'S', 'T', 'S')\n",
      "GRIPPED None ('G', 'R', 'IH', 'P', 'T')\n",
      "INTERSPERSES None ('IH', 'N', 'T', 'ER', 'S', 'P', 'ER', 'S', 'AH', 'Z')\n",
      "LIMESTONE None ('L', 'AY', 'M', 'S', 'T', 'OW', 'N')\n",
      "MONROEVILLE None ('M', 'AA', 'N', 'R', 'OW', 'V', 'IH', 'L')\n",
      "PENDERGAST None ('P', 'EH', 'N', 'D', 'ER', 'G', 'AE', 'S', 'T')\n",
      "REPEATS 1 ('R', 'IY', 'P', 'IY', 'T', 'S')\n",
      "SIELAFF None ('S', 'IY', 'L', 'AE', 'F')\n",
      "THIBEDEAU None ('TH', 'IH', 'B', 'IH', 'D', 'OW')\n",
      "WHITEAKER 1 ('HH', 'W', 'IH', 'T', 'AH', 'K', 'ER')\n"
     ]
    }
   ],
   "source": [
    "def strip_stress(phone: str) -> str:\n",
    "    _m = re.match(r'(.*)\\d+$', phone)\n",
    "    if _m is not None:\n",
    "        phone = _m.group(1)\n",
    "    return phone\n",
    "\n",
    "# test \n",
    "for l in entry_lines[5::10000]:\n",
    "    _word, _pron = l.split('  ')\n",
    "    \n",
    "    if _word.endswith(')'):\n",
    "        # variant\n",
    "        _m = re.match(r'(.*)\\((.*)\\)$', _word)\n",
    "        _word = _m.group(1)\n",
    "        _variant = _m.group(2)\n",
    "    else:\n",
    "        # no variant\n",
    "        _variant = None\n",
    "    \n",
    "    _phones = tuple([strip_stress(_phone) for _phone in _pron.split(' ')])\n",
    "    \n",
    "    print(_word, _variant, _phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fd9c9a43504ab0ae8233d1a562afb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=133854), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(125074, 113745)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2pron = defaultdict(set) # all possible prons, Dict[str, Set[Tuple[str]]]\n",
    "pron2word = defaultdict(set) # all possible words, Dict[Tuple[str], Set[str]]\n",
    "\n",
    "for l in tqdm(entry_lines):\n",
    "    _word, _pron = l.split('  ')\n",
    "    \n",
    "    if _word.endswith(')'):\n",
    "        # variant\n",
    "        _m = re.match(r'(.*)\\((.*)\\)$', _word)\n",
    "        _word = _m.group(1)\n",
    "        _variant = _m.group(2)\n",
    "    else:\n",
    "        # no variant\n",
    "        _variant = None\n",
    "    \n",
    "    _phones = tuple([strip_stress(_phone) for _phone in _pron.split(' ')])\n",
    "    \n",
    "    word2pron[_word].add(_phones)\n",
    "    pron2word[_phones].add(_word)\n",
    "\n",
    "len(word2pron), len(pron2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125074, 125074, 113745)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word = list(word2pron.keys())\n",
    "word2idx = {w : idx for idx, w in enumerate(idx2word)}\n",
    "word2pieces = {w : bert_tokenizer.tokenize(w) for w in idx2word}\n",
    "idx2pron = list(pron2word.keys())\n",
    "pron2idx = {p : idx for idx, p in enumerate(idx2pron)}\n",
    "len(word2idx), len(word2pieces), len(pron2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('R', 'AH', 'K', 'AO', 'R', 'D'),\n",
       " ('R', 'EH', 'K', 'ER', 'D'),\n",
       " ('R', 'IH', 'K', 'AO', 'R', 'D')}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2pron['RECORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91994, 'RECORD')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['RECORD'], idx2word[91994]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83079, ('R', 'EH', 'D'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pron2idx[('R', 'EH', 'D')], idx2pron[83079]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suffixes\n",
    "suffixes = [\n",
    "    ['S'],\n",
    "    ['IY NG'],\n",
    "    ['IH NG'],\n",
    "    ['D']\n",
    "]\n",
    "\n",
    "# Similar phone clusters \n",
    "clusters = [\n",
    "    ['Z', 'S'],\n",
    "    ['AA', 'AO', 'EY', 'UH'],\n",
    "    ['AXR', 'AX'],\n",
    "    ['P', 'B', 'F'],\n",
    "    ['DH', 'CH', 'ZH', 'T', 'SH'],\n",
    "    ['IY', 'AY', 'OW'],\n",
    "    ['EH', 'AH', 'IH', 'AW', 'ER', 'UW']\n",
    "]\n",
    "\n",
    "phone2cluster = defaultdict(list) # Dict[str, List(str)]\n",
    "\n",
    "for c in clusters:\n",
    "    for p in c:\n",
    "        phone2cluster[p] = c\n",
    "\n",
    "len(phone2cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IY', 'AY', 'OW']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone2cluster['IY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37a6ef2b7b44fb1baa8edcb1289f386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove consonant: QUOTE('K', 'W', 'OW', 'T') -> COAT('K', 'OW', 'T')\n",
      "Remove consonant: QUOTE('K', 'W', 'OW', 'T') -> QUO('K', 'W', 'OW')\n",
      "Add suffix: QUOTE('K', 'W', 'OW', 'T') -> QUOTES('K', 'W', 'OW', 'T', 'S')\n",
      "Substitute phone: QUOTE('K', 'W', 'OW', 'T') -> QUIETT('K', 'W', 'IY', 'T')\n",
      "Substitute phone: QUOTE('K', 'W', 'OW', 'T') -> QUITE('K', 'W', 'AY', 'T')\n",
      "Remove consonant: \"UNQUOTE('AH', 'N', 'K', 'W', 'OW', 'T') -> UNCOAT('AH', 'N', 'K', 'OW', 'T')\n",
      "Remove consonant: PERCENT('P', 'ER', 'S', 'EH', 'N', 'T') -> PERSET('P', 'ER', 'S', 'EH', 'T')\n",
      "Add suffix: PERCENT('P', 'ER', 'S', 'EH', 'N', 'T') -> PERCENTS('P', 'ER', 'S', 'EH', 'N', 'T', 'S')\n",
      "Substitute phone: PERCENT('P', 'ER', 'S', 'EH', 'N', 'T') -> PRESENT('P', 'ER', 'Z', 'EH', 'N', 'T')\n",
      "Substitute phone: 'ALLO('AA', 'L', 'OW') -> OLLY('AA', 'L', 'IY')\n",
      "Remove consonant: BOUT('B', 'AW', 'T') -> OUT('AW', 'T')\n",
      "Add suffix: BOUT('B', 'AW', 'T') -> BOUTS('B', 'AW', 'T', 'S')\n",
      "Substitute phone: BOUT('B', 'AW', 'T') -> POUT('P', 'AW', 'T')\n",
      "Add suffix: KAY('K', 'EY') -> CAYCE('K', 'EY', 'S')\n",
      "Add suffix: KAY('K', 'EY') -> KADE('K', 'EY', 'D')\n",
      "Double vowel: A('EY',) -> AA('EY', 'EY')\n",
      "Double vowel: ACQUIRE('AH', 'K', 'W', 'AY', 'ER') -> ACQUIRER('AH', 'K', 'W', 'AY', 'ER', 'ER')\n",
      "Double vowel: ACQUIRES('AH', 'K', 'W', 'AY', 'ER', 'Z') -> ACQUIRERS('AH', 'K', 'W', 'AY', 'ER', 'ER', 'Z')\n",
      "Double vowel: ADVENTURE('AE', 'D', 'V', 'EH', 'N', 'CH', 'ER') -> ADVENTURER('AE', 'D', 'V', 'EH', 'N', 'CH', 'ER', 'ER')\n",
      "Double vowel: ADVENTURE('AH', 'D', 'V', 'EH', 'N', 'CH', 'ER') -> ADVENTURER('AH', 'D', 'V', 'EH', 'N', 'CH', 'ER', 'ER')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1-step confusion \n",
    "\n",
    "# confusable pron ids, List[int]; [i] has j: i can be replaced by j \n",
    "# including self, i.e. [i] always has i  \n",
    "pron_confusions = [set([idx]) for idx in range(len(idx2pron))]\n",
    "\n",
    "remove_consonant_cnt = 0\n",
    "double_vowel_cnt = 0\n",
    "add_suffix_cnt = 0\n",
    "substitute_cnt = 0\n",
    "\n",
    "for _idx, _pron in tqdm(enumerate(idx2pron), total=len(idx2pron)):\n",
    "    _phones = list(_pron)\n",
    "    \n",
    "    # remove a consonant (outgoing edge only)\n",
    "    for j in range(len(_phones)):\n",
    "        if _phones[j][0] in 'AEIOU':\n",
    "            continue\n",
    "            \n",
    "        _confs_pron = tuple(_phones[:j] + _phones[j+1:])\n",
    "        try:\n",
    "            _confs_idx = pron2idx[_confs_pron]\n",
    "            pron_confusions[_idx].add(_confs_idx)\n",
    "            remove_consonant_cnt += 1\n",
    "            \n",
    "            if remove_consonant_cnt <= 5:\n",
    "                _src_pron = _pron\n",
    "                _src_w = next(iter(pron2word[_src_pron]))\n",
    "                _tgt_pron = _confs_pron\n",
    "                _tgt_w = next(iter(pron2word[_tgt_pron]))\n",
    "                print(f'Remove consonant: {_src_w}{_src_pron} -> {_tgt_w}{_tgt_pron}')\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # remove a doubled vowel (incoming edge only, for doubling vowel)\n",
    "    for j in range(1, len(_phones)):\n",
    "        if _phones[j] != _phones[j-1] or (not _phones[j][0] in 'AEIOU'):\n",
    "            continue\n",
    "            \n",
    "        _confs_pron = tuple(_phones[:j] + _phones[j+1:])\n",
    "        try:\n",
    "            _confs_idx = pron2idx[_confs_pron]\n",
    "            pron_confusions[_confs_idx].add(_idx)\n",
    "            double_vowel_cnt += 1\n",
    "            \n",
    "            if double_vowel_cnt <= 5:\n",
    "                _src_pron = _confs_pron\n",
    "                _src_w = next(iter(pron2word[_src_pron]))\n",
    "                _tgt_pron = _pron\n",
    "                _tgt_w = next(iter(pron2word[_tgt_pron]))\n",
    "                print(f'Double vowel: {_src_w}{_src_pron} -> {_tgt_w}{_tgt_pron}')\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # add a suffix (outgoing edge only)\n",
    "    for _suffix in suffixes:\n",
    "        _confs_pron = tuple(_phones + _suffix)\n",
    "        try:\n",
    "            _confs_idx = pron2idx[_confs_pron]\n",
    "            pron_confusions[_idx].add(_confs_idx)\n",
    "            add_suffix_cnt += 1\n",
    "            \n",
    "            if add_suffix_cnt <= 5:\n",
    "                _src_pron = _pron\n",
    "                _src_w = next(iter(pron2word[_src_pron]))\n",
    "                _tgt_pron = _confs_pron\n",
    "                _tgt_w = next(iter(pron2word[_tgt_pron]))\n",
    "                print(f'Add suffix: {_src_w}{_src_pron} -> {_tgt_w}{_tgt_pron}')\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # substitute a phone (outgoing edge only; it's bidirectional in essense)\n",
    "    for j in range(len(_phones)):\n",
    "        _ph = _phones[j]\n",
    "        _ph_cluster = phone2cluster[_ph]\n",
    "        if len(_ph_cluster) <= 1:\n",
    "            continue\n",
    "        \n",
    "        # has a cluster, do replace\n",
    "        for _confs_ph in _ph_cluster:\n",
    "            if _confs_ph == _ph:\n",
    "                continue\n",
    "            \n",
    "            _confs_pron = tuple(_phones[:j] + [_confs_ph] + _phones[j+1:])\n",
    "            try:\n",
    "                _confs_idx = pron2idx[_confs_pron]\n",
    "                pron_confusions[_idx].add(_confs_idx)\n",
    "                substitute_cnt += 1\n",
    "\n",
    "                if substitute_cnt <= 5:\n",
    "                    _src_pron = _pron\n",
    "                    _src_w = next(iter(pron2word[_src_pron]))\n",
    "                    _tgt_pron = _confs_pron\n",
    "                    _tgt_w = next(iter(pron2word[_tgt_pron]))\n",
    "                    print(f'Substitute phone: {_src_w}{_src_pron} -> {_tgt_w}{_tgt_pron}')\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65833, 86, 7700, 51210)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_consonant_cnt, double_vowel_cnt, add_suffix_cnt, substitute_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 51250),\n",
       " (2, 32982),\n",
       " (3, 13536),\n",
       " (4, 7222),\n",
       " (5, 4249),\n",
       " (6, 2400),\n",
       " (7, 1154),\n",
       " (8, 613),\n",
       " (9, 236),\n",
       " (10, 76),\n",
       " (11, 14),\n",
       " (12, 11),\n",
       " (13, 2)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter([len(x) for x in pron_confusions]).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ru', '##ud']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize('RUUD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f8593854a34572a05350ceb7f3a0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125074), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Word confusion \n",
    "word_confusions = defaultdict(set) # confusable words (not ids!), Dict[str, Set[str]]\n",
    "\n",
    "for _word in tqdm(idx2word):\n",
    "    _word_pieces = word2pieces[_word]\n",
    "    \n",
    "    for _pron in word2pron[_word]:\n",
    "        _pron_idx = pron2idx[_pron]\n",
    "        _confs_pron_ids = pron_confusions[_pron_idx]\n",
    "        for _confs_pron_idx in _confs_pron_ids:\n",
    "            _confs_pron = idx2pron[_confs_pron_idx]\n",
    "            _confs_words = pron2word[_confs_pron]\n",
    "            \n",
    "            for w in _confs_words:\n",
    "                if len(word2pieces[w]) != len(_word_pieces): continue\n",
    "                if w == _word: continue\n",
    "                word_confusions[_word].add(w)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the word_confusions dict \n",
    "# word_confusions_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/TaBERT/data/word_confusions.pkl'\n",
    "\n",
    "# with open(word_confusions_path, 'wb') as f:\n",
    "#     pickle.dump(word_confusions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word_confusions dict \n",
    "word_confusions_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/TaBERT/data/word_confusions.pkl'\n",
    "\n",
    "with open(word_confusions_path, 'rb') as f:\n",
    "    word_confusions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 69467),\n",
       " (1, 24437),\n",
       " (2, 9617),\n",
       " (3, 6079),\n",
       " (4, 4126),\n",
       " (5, 3007),\n",
       " (6, 2207),\n",
       " (7, 1615),\n",
       " (8, 1096),\n",
       " (9, 920),\n",
       " (10, 641),\n",
       " (11, 447),\n",
       " (12, 388),\n",
       " (13, 258),\n",
       " (14, 222),\n",
       " (15, 144),\n",
       " (16, 136),\n",
       " (17, 75),\n",
       " (18, 74),\n",
       " (19, 40),\n",
       " (20, 26),\n",
       " (21, 13),\n",
       " (22, 3),\n",
       " (23, 19),\n",
       " (24, 7),\n",
       " (25, 2),\n",
       " (27, 3),\n",
       " (28, 2),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (34, 1)]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter([len(word_confusions[w]) for w in idx2word]).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('B', 'AY')} ['bae']\n",
      "[('BI', ['bi']), ('B', ['b']), ('PIE', ['pie']), ('BEE', ['bee']), ('I', ['i']), ('EYE', ['eye']), ('BY', ['by']), ('PHI', ['phi']), ('BYE', ['bye']), ('FI', ['fi']), ('BUY', ['buy']), ('BE', ['be']), ('BOW', ['bow']), ('PI', ['pi']), ('BEA', ['bea']), ('AI', ['ai']), ('BO', ['bo']), ('BEAU', ['beau']), ('FAE', ['fae']), ('AYE', ['aye']), ('BEAUX', ['beaux'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'IY', 'K'), ('B', 'EH', 'K')} ['bae', '##k']\n",
      "[('BUC', ['bu', '##c']), ('BOAKE', ['bo', '##ake']), ('BECKS', ['beck', '##s']), ('BIC', ['bi', '##c']), ('PAEK', ['pa', '##ek']), ('BEEKS', ['bee', '##ks']), ('BOUCK', ['bo', '##uck']), ('BICK', ['bi', '##ck']), ('B.', ['b', '.']), ('EKE', ['ek', '##e']), ('FECK', ['fe', '##ck']), ('BOEKE', ['bo', '##eke']), ('BIRK', ['bi', '##rk']), ('PIQUE', ['pi', '##que']), ('BOOCK', ['boo', '##ck']), ('BERCH', ['be', '##rch']), ('PECH', ['pe', '##ch']), ('BURK', ['bu', '##rk']), ('BEEK', ['bee', '##k']), ('BERKE', ['be', '##rke']), ('ECK', ['ec', '##k']), ('PEAKE', ['peak', '##e']), ('BEECK', ['bee', '##ck']), ('BOAK', ['bo', '##ak']), ('BOECK', ['bo', '##eck']), ('BEC', ['be', '##c']), ('FEICK', ['fei', '##ck']), ('BERK', ['be', '##rk'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'AA', 'R'), ('B', 'EH', 'R')} ['ba', '##hr']\n",
      "[('BOOR', ['boo', '##r']), ('ERR', ['er', '##r']), ('ERE', ['er', '##e']), ('FERRE', ['fe', '##rre']), ('FEHR', ['fe', '##hr']), ('FAHR', ['fa', '##hr']), ('FARR', ['far', '##r']), ('PARE', ['par', '##e']), ('BERE', ['be', '##re']), ('BEARE', ['bear', '##e']), ('BAH', ['ba', '##h']), ('FAIRE', ['fair', '##e']), ('AYRE', ['a', '##yre']), ('BAUR', ['ba', '##ur']), ('BAHRE', ['ba', '##hre']), ('PARR', ['par', '##r']), ('BORRE', ['bo', '##rre']), ('PHAIR', ['ph', '##air']), ('AHR', ['ah', '##r']), ('BAAR', ['ba', '##ar']), ('BAIR', ['bai', '##r']), ('BEHR', ['be', '##hr']), ('PHAR', ['ph', '##ar']), ('BOHR', ['bo', '##hr']), ('PAAR', ['pa', '##ar']), ('FER', ['fe', '##r']), ('BAER', ['bae', '##r']), ('AER', ['ae', '##r']), ('R.', ['r', '.']), ('BAEHR', ['bae', '##hr']), ('BOUR', ['bo', '##ur'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'EY', 'L', 'Z')} ['bail', '##es']\n",
      "[('ALES', ['ale', '##s']), ('PALES', ['pale', '##s']), ('BALE', ['bal', '##e']), ('BAILE', ['bail', '##e']), ('BAYLES', ['bay', '##les']), ('BAIZE', ['bai', '##ze']), ('BALLE', ['ball', '##e']), ('BALES', ['bal', '##es']), ('BALLES', ['ball', '##es']), ('AYLES', ['a', '##yles']), ('BAYES', ['bay', '##es']), ('BEYL', ['bey', '##l']), ('BAISE', ['bai', '##se']), ('AILES', ['ai', '##les']), ('BAYSE', ['bays', '##e']), ('PAILS', ['pa', '##ils']), ('BAYLE', ['bay', '##le']), ('BALZ', ['bal', '##z']), ('BAZE', ['ba', '##ze']), ('BAILS', ['bail', '##s']), ('AILS', ['ai', '##ls']), ('FALES', ['fa', '##les']), ('ALLES', ['all', '##es']), (\"BULLS'\", ['bulls', \"'\"])]\n",
      "--------------------------------------------------\n",
      "{('B', 'EY', 'L', 'Z')} ['bail', '##s']\n",
      "[('BAILES', ['bail', '##es']), ('ALES', ['ale', '##s']), ('PALES', ['pale', '##s']), ('BALE', ['bal', '##e']), ('BAILE', ['bail', '##e']), ('BAYLES', ['bay', '##les']), ('BAIZE', ['bai', '##ze']), ('BALLE', ['ball', '##e']), ('BALES', ['bal', '##es']), ('BALLES', ['ball', '##es']), ('AYLES', ['a', '##yles']), ('BAYES', ['bay', '##es']), ('BEYL', ['bey', '##l']), ('BAISE', ['bai', '##se']), ('AILES', ['ai', '##les']), ('BAYSE', ['bays', '##e']), ('PAILS', ['pa', '##ils']), ('BAYLE', ['bay', '##le']), ('BALZ', ['bal', '##z']), ('BAZE', ['ba', '##ze']), ('AILS', ['ai', '##ls']), ('FALES', ['fa', '##les']), ('ALLES', ['all', '##es']), (\"BULLS'\", ['bulls', \"'\"])]\n",
      "--------------------------------------------------\n",
      "{('B', 'EY', 'L', 'Z')} ['bal', '##es']\n",
      "[('BAILES', ['bail', '##es']), ('ALES', ['ale', '##s']), ('PALES', ['pale', '##s']), ('BALE', ['bal', '##e']), ('BAILE', ['bail', '##e']), ('BAYLES', ['bay', '##les']), ('BAIZE', ['bai', '##ze']), ('BALLE', ['ball', '##e']), ('BALLES', ['ball', '##es']), ('AYLES', ['a', '##yles']), ('BAYES', ['bay', '##es']), ('BEYL', ['bey', '##l']), ('BAISE', ['bai', '##se']), ('AILES', ['ai', '##les']), ('BAYSE', ['bays', '##e']), ('PAILS', ['pa', '##ils']), ('BAYLE', ['bay', '##le']), ('BALZ', ['bal', '##z']), ('BAZE', ['ba', '##ze']), ('BAILS', ['bail', '##s']), ('AILS', ['ai', '##ls']), ('FALES', ['fa', '##les']), ('ALLES', ['all', '##es']), (\"BULLS'\", ['bulls', \"'\"])]\n",
      "--------------------------------------------------\n",
      "{('B', 'EY', 'L', 'Z')} ['ball', '##es']\n",
      "[('BAILES', ['bail', '##es']), ('ALES', ['ale', '##s']), ('PALES', ['pale', '##s']), ('BALE', ['bal', '##e']), ('BAILE', ['bail', '##e']), ('BAYLES', ['bay', '##les']), ('BAIZE', ['bai', '##ze']), ('BALLE', ['ball', '##e']), ('BALES', ['bal', '##es']), ('AYLES', ['a', '##yles']), ('BAYES', ['bay', '##es']), ('BEYL', ['bey', '##l']), ('BAISE', ['bai', '##se']), ('AILES', ['ai', '##les']), ('BAYSE', ['bays', '##e']), ('PAILS', ['pa', '##ils']), ('BAYLE', ['bay', '##le']), ('BALZ', ['bal', '##z']), ('BAZE', ['ba', '##ze']), ('BAILS', ['bail', '##s']), ('AILS', ['ai', '##ls']), ('FALES', ['fa', '##les']), ('ALLES', ['all', '##es']), (\"BULLS'\", ['bulls', \"'\"])]\n",
      "--------------------------------------------------\n",
      "{('B', 'AA', 'R'), ('B', 'EH', 'R', 'IY')} ['barre']\n",
      "[('BARRED', ['barred']), ('OUR', ['our']), ('BURIED', ['buried']), ('FAR', ['far']), ('PARRY', ['parry']), ('R', ['r']), ('BARROW', ['barrow']), ('ARE', ['are']), ('BA', ['ba']), ('BARD', ['bard']), ('BARR', ['barr']), ('BERRY', ['berry']), ('BARRIE', ['barrie']), ('FAIRY', ['fairy']), ('BOER', ['boer']), ('BURY', ['bury']), ('BAR', ['bar']), ('FERRY', ['ferry']), ('BORE', ['bore']), ('PERRY', ['perry']), ('PAR', ['par']), ('BOAR', ['boar']), ('AR', ['ar']), ('BARRY', ['barry'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'AW', 'ER'), ('B', 'AO', 'R')} ['ba', '##ur']\n",
      "[('BOOR', ['boo', '##r']), ('BAUGH', ['ba', '##ugh']), ('BURRER', ['burr', '##er']), ('BUER', ['bu', '##er']), ('BAHRE', ['ba', '##hre']), ('BORRE', ['bo', '##rre']), ('AUER', ['au', '##er']), ('BAAR', ['ba', '##ar']), ('BOWER', ['bow', '##er']), ('BORSE', ['bo', '##rse']), ('OAR', ['o', '##ar']), ('OHR', ['oh', '##r']), ('PORE', ['por', '##e']), ('FORR', ['for', '##r']), ('BOHR', ['bo', '##hr']), ('FAURE', ['fa', '##ure']), ('BOOHER', ['boo', '##her']), ('PORR', ['por', '##r']), ('HORS', ['ho', '##rs']), ('BOURSE', ['bo', '##urse']), ('BAHR', ['ba', '##hr'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'EY', 'L', 'Z')} ['bay', '##les']\n",
      "[('BAILES', ['bail', '##es']), ('ALES', ['ale', '##s']), ('PALES', ['pale', '##s']), ('BALE', ['bal', '##e']), ('BAILE', ['bail', '##e']), ('BAIZE', ['bai', '##ze']), ('BALLE', ['ball', '##e']), ('BALES', ['bal', '##es']), ('BALLES', ['ball', '##es']), ('AYLES', ['a', '##yles']), ('BAYES', ['bay', '##es']), ('BEYL', ['bey', '##l']), ('BAISE', ['bai', '##se']), ('AILES', ['ai', '##les']), ('BAYSE', ['bays', '##e']), ('PAILS', ['pa', '##ils']), ('BAYLE', ['bay', '##le']), ('BALZ', ['bal', '##z']), ('BAZE', ['ba', '##ze']), ('BAILS', ['bail', '##s']), ('AILS', ['ai', '##ls']), ('FALES', ['fa', '##les']), ('ALLES', ['all', '##es']), (\"BULLS'\", ['bulls', \"'\"])]\n",
      "--------------------------------------------------\n",
      "{('B', 'OW')} ['beau']\n",
      "[('FOE', ['foe']), ('BI', ['bi']), ('B', ['b']), ('O', ['o']), ('BAE', ['bae']), ('BEE', ['bee']), ('BY', ['by']), ('BOWED', ['bowed']), ('AU', ['au']), ('AUX', ['aux']), ('OWE', ['owe']), ('BYE', ['bye']), ('BUY', ['buy']), ('BE', ['be']), ('BOW', ['bow']), ('BEA', ['bea']), ('BO', ['bo']), ('PO', ['po']), ('POE', ['poe']), ('FAUX', ['faux']), ('OH', ['oh']), ('OW', ['ow']), ('BEAUX', ['beaux'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'OW')} ['beaux']\n",
      "[('FOE', ['foe']), ('BI', ['bi']), ('B', ['b']), ('O', ['o']), ('BAE', ['bae']), ('BEE', ['bee']), ('BY', ['by']), ('BOWED', ['bowed']), ('AU', ['au']), ('AUX', ['aux']), ('OWE', ['owe']), ('BYE', ['bye']), ('BUY', ['buy']), ('BE', ['be']), ('BOW', ['bow']), ('BEA', ['bea']), ('BO', ['bo']), ('PO', ['po']), ('POE', ['poe']), ('BEAU', ['beau']), ('FAUX', ['faux']), ('OH', ['oh']), ('OW', ['ow'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'EY', 'T'), ('B', 'AY', 'T')} ['beit']\n",
      "[('BI', ['bi']), ('BAE', ['bae']), ('BYTES', ['bytes']), ('BATES', ['bates']), ('FATE', ['fate']), ('FIGHT', ['fight']), ('BOUGHT', ['bought']), ('BY', ['by']), ('BAY', ['bay']), ('BEIGE', ['beige']), ('BITES', ['bites']), ('BYE', ['bye']), ('BUY', ['buy']), ('BITE', ['bite']), ('BYTE', ['byte']), ('ATE', ['ate']), ('EIGHT', ['eight']), ('BOAT', ['boat']), ('BAIT', ['bait']), ('BEAT', ['beat']), ('BEY', ['bey'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'ER', 'K')} ['be', '##rch']\n",
      "[('BUC', ['bu', '##c']), ('PERK', ['per', '##k']), ('BIC', ['bi', '##c']), ('IRK', ['ir', '##k']), ('PERC', ['per', '##c']), ('BIRR', ['bi', '##rr']), ('BICK', ['bi', '##ck']), ('BIRKS', ['bi', '##rks']), ('BURKES', ['burke', '##s']), ('BURKS', ['bu', '##rks']), ('BUR', ['bu', '##r']), ('BIRK', ['bi', '##rk']), ('ERK', ['er', '##k']), ('FERCH', ['fe', '##rch']), ('BOOCK', ['boo', '##ck']), ('BURK', ['bu', '##rk']), ('ERCK', ['er', '##ck']), ('BERKE', ['be', '##rke']), ('BIR', ['bi', '##r']), ('PURK', ['pu', '##rk']), ('BAEK', ['bae', '##k']), ('BEC', ['be', '##c']), ('BERK', ['be', '##rk'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'ER', 'K')} ['be', '##rk']\n",
      "[('BUC', ['bu', '##c']), ('PERK', ['per', '##k']), ('BIC', ['bi', '##c']), ('IRK', ['ir', '##k']), ('PERC', ['per', '##c']), ('BIRR', ['bi', '##rr']), ('BICK', ['bi', '##ck']), ('BIRKS', ['bi', '##rks']), ('BURKES', ['burke', '##s']), ('BURKS', ['bu', '##rks']), ('BUR', ['bu', '##r']), ('BIRK', ['bi', '##rk']), ('ERK', ['er', '##k']), ('FERCH', ['fe', '##rch']), ('BOOCK', ['boo', '##ck']), ('BERCH', ['be', '##rch']), ('BURK', ['bu', '##rk']), ('ERCK', ['er', '##ck']), ('BERKE', ['be', '##rke']), ('BIR', ['bi', '##r']), ('PURK', ['pu', '##rk']), ('BAEK', ['bae', '##k']), ('BEC', ['be', '##c'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'ER', 'K')} ['be', '##rke']\n",
      "[('BUC', ['bu', '##c']), ('PERK', ['per', '##k']), ('BIC', ['bi', '##c']), ('IRK', ['ir', '##k']), ('PERC', ['per', '##c']), ('BIRR', ['bi', '##rr']), ('BICK', ['bi', '##ck']), ('BIRKS', ['bi', '##rks']), ('BURKES', ['burke', '##s']), ('BURKS', ['bu', '##rks']), ('BUR', ['bu', '##r']), ('BIRK', ['bi', '##rk']), ('ERK', ['er', '##k']), ('FERCH', ['fe', '##rch']), ('BOOCK', ['boo', '##ck']), ('BERCH', ['be', '##rch']), ('BURK', ['bu', '##rk']), ('ERCK', ['er', '##ck']), ('BIR', ['bi', '##r']), ('PURK', ['pu', '##rk']), ('BAEK', ['bae', '##k']), ('BEC', ['be', '##c']), ('BERK', ['be', '##rk'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'AY')} ['bi']\n",
      "[('B', ['b']), ('PIE', ['pie']), ('BAE', ['bae']), ('BEE', ['bee']), ('I', ['i']), ('EYE', ['eye']), ('BY', ['by']), ('PHI', ['phi']), ('BYE', ['bye']), ('FI', ['fi']), ('BUY', ['buy']), ('BE', ['be']), ('BOW', ['bow']), ('PI', ['pi']), ('BEA', ['bea']), ('AI', ['ai']), ('BO', ['bo']), ('BEAU', ['beau']), ('FAE', ['fae']), ('AYE', ['aye']), ('BEAUX', ['beaux'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'ER', 'K')} ['bi', '##rk']\n",
      "[('BUC', ['bu', '##c']), ('PERK', ['per', '##k']), ('BIC', ['bi', '##c']), ('IRK', ['ir', '##k']), ('PERC', ['per', '##c']), ('BIRR', ['bi', '##rr']), ('BICK', ['bi', '##ck']), ('BIRKS', ['bi', '##rks']), ('BURKES', ['burke', '##s']), ('BURKS', ['bu', '##rks']), ('BUR', ['bu', '##r']), ('ERK', ['er', '##k']), ('FERCH', ['fe', '##rch']), ('BOOCK', ['boo', '##ck']), ('BERCH', ['be', '##rch']), ('BURK', ['bu', '##rk']), ('ERCK', ['er', '##ck']), ('BERKE', ['be', '##rke']), ('BIR', ['bi', '##r']), ('PURK', ['pu', '##rk']), ('BAEK', ['bae', '##k']), ('BEC', ['be', '##c']), ('BERK', ['be', '##rk'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'OW')} ['bo']\n",
      "[('FOE', ['foe']), ('BI', ['bi']), ('B', ['b']), ('O', ['o']), ('BAE', ['bae']), ('BEE', ['bee']), ('BY', ['by']), ('BOWED', ['bowed']), ('AU', ['au']), ('AUX', ['aux']), ('OWE', ['owe']), ('BYE', ['bye']), ('BUY', ['buy']), ('BE', ['be']), ('BOW', ['bow']), ('BEA', ['bea']), ('PO', ['po']), ('POE', ['poe']), ('BEAU', ['beau']), ('FAUX', ['faux']), ('OH', ['oh']), ('OW', ['ow']), ('BEAUX', ['beaux'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'AW'), ('B', 'OW')} ['bow']\n",
      "[('FOE', ['foe']), ('BI', ['bi']), ('B', ['b']), ('O', ['o']), ('BAE', ['bae']), ('BEE', ['bee']), ('BY', ['by']), ('BOWED', ['bowed']), ('AU', ['au']), ('AUX', ['aux']), ('OWE', ['owe']), ('BYE', ['bye']), ('BUY', ['buy']), ('BE', ['be']), ('BEA', ['bea']), ('BAO', ['bao']), ('BO', ['bo']), ('PO', ['po']), ('BOO', ['boo']), ('POE', ['poe']), ('BEAU', ['beau']), ('FAUX', ['faux']), ('POW', ['pow']), ('OH', ['oh']), ('OW', ['ow']), ('BURR', ['burr']), ('BEAUX', ['beaux'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'R', 'UW'), ('B', 'R', 'OW')} ['br', '##eaux']\n",
      "[('RHO', ['r', '##ho']), ('BOWE', ['bow', '##e']), ('ROHE', ['ro', '##he']), ('RHUE', ['r', '##hue']), ('RHEAULT', ['rhea', '##ult']), ('BOE', ['bo', '##e']), ('BRUE', ['br', '##ue']), ('REW', ['re', '##w']), ('PRUGH', ['pr', '##ugh']), ('ROUX', ['ro', '##ux']), ('REAUX', ['re', '##aux']), ('BRYE', ['br', '##ye']), ('FREW', ['fr', '##ew']), ('BREA', ['br', '##ea']), ('BRUGH', ['br', '##ugh']), ('BROCE', ['bro', '##ce']), ('PRU', ['pr', '##u']), ('PREW', ['pre', '##w']), ('RIOUX', ['rio', '##ux']), ('FRO', ['fr', '##o']), ('ROH', ['ro', '##h']), ('WROE', ['wr', '##oe']), ('BROE', ['bro', '##e']), ('BOEH', ['bo', '##eh']), ('FROH', ['fr', '##oh']), ('BREAU', ['br', '##eau']), ('BRODE', ['bro', '##de']), ('BROUGH', ['bro', '##ugh']), ('BEU', ['be', '##u']), ('BREWED', ['brew', '##ed']), ('BROWE', ['brow', '##e']), ('BRAU', ['bra', '##u']), ('PRUE', ['pr', '##ue']), ('BRIE', ['br', '##ie'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'ER', 'K')} ['bu', '##rk']\n",
      "[('BUC', ['bu', '##c']), ('PERK', ['per', '##k']), ('BIC', ['bi', '##c']), ('IRK', ['ir', '##k']), ('PERC', ['per', '##c']), ('BIRR', ['bi', '##rr']), ('BICK', ['bi', '##ck']), ('BIRKS', ['bi', '##rks']), ('BURKES', ['burke', '##s']), ('BURKS', ['bu', '##rks']), ('BUR', ['bu', '##r']), ('BIRK', ['bi', '##rk']), ('ERK', ['er', '##k']), ('FERCH', ['fe', '##rch']), ('BOOCK', ['boo', '##ck']), ('BERCH', ['be', '##rch']), ('ERCK', ['er', '##ck']), ('BERKE', ['be', '##rke']), ('BIR', ['bi', '##r']), ('PURK', ['pu', '##rk']), ('BAEK', ['bae', '##k']), ('BEC', ['be', '##c']), ('BERK', ['be', '##rk'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'AY')} ['buy']\n",
      "[('BI', ['bi']), ('B', ['b']), ('PIE', ['pie']), ('BAE', ['bae']), ('BEE', ['bee']), ('I', ['i']), ('EYE', ['eye']), ('BY', ['by']), ('PHI', ['phi']), ('BYE', ['bye']), ('FI', ['fi']), ('BE', ['be']), ('BOW', ['bow']), ('PI', ['pi']), ('BEA', ['bea']), ('AI', ['ai']), ('BO', ['bo']), ('BEAU', ['beau']), ('FAE', ['fae']), ('AYE', ['aye']), ('BEAUX', ['beaux'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'AY')} ['by']\n",
      "[('BI', ['bi']), ('B', ['b']), ('PIE', ['pie']), ('BAE', ['bae']), ('BEE', ['bee']), ('I', ['i']), ('EYE', ['eye']), ('PHI', ['phi']), ('BYE', ['bye']), ('FI', ['fi']), ('BUY', ['buy']), ('BE', ['be']), ('BOW', ['bow']), ('PI', ['pi']), ('BEA', ['bea']), ('AI', ['ai']), ('BO', ['bo']), ('BEAU', ['beau']), ('FAE', ['fae']), ('AYE', ['aye']), ('BEAUX', ['beaux'])]\n",
      "--------------------------------------------------\n",
      "{('B', 'AY')} ['bye']\n",
      "[('BI', ['bi']), ('B', ['b']), ('PIE', ['pie']), ('BAE', ['bae']), ('BEE', ['bee']), ('I', ['i']), ('EYE', ['eye']), ('BY', ['by']), ('PHI', ['phi']), ('FI', ['fi']), ('BUY', ['buy']), ('BE', ['be']), ('BOW', ['bow']), ('PI', ['pi']), ('BEA', ['bea']), ('AI', ['ai']), ('BO', ['bo']), ('BEAU', ['beau']), ('FAE', ['fae']), ('AYE', ['aye']), ('BEAUX', ['beaux'])]\n",
      "--------------------------------------------------\n",
      "{('K', 'OW'), ('K', 'AW')} ['ca', '##u']\n",
      "[('KOH', ['ko', '##h']), ('OHH', ['oh', '##h']), ('AUE', ['au', '##e']), ('COAD', ['coa', '##d']), (\"O'\", ['o', \"'\"]), ('COED', ['coe', '##d']), ('COO', ['co', '##o']), ('COU', ['co', '##u']), ('KUHR', ['ku', '##hr']), ('O.', ['o', '.']), ('KEE', ['ke', '##e']), ('CUR', ['cu', '##r']), ('COUSE', ['co', '##use']), ('COWED', ['cow', '##ed']), ('KAO', ['ka', '##o']), ('KEA', ['ke', '##a']), ('KOUGH', ['ko', '##ugh']), ('KEYE', ['key', '##e']), ('KOO', ['ko', '##o']), ('KER', ['ke', '##r']), ('KAU', ['ka', '##u']), ('KYI', ['ky', '##i']), ('CO.', ['co', '.']), ('EAUX', ['ea', '##ux']), ('EAU', ['ea', '##u'])]\n",
      "--------------------------------------------------\n",
      "{('D', 'EY'), ('D', 'IY'), ('D', 'AH')} ['de']\n",
      "[('DEE', ['dee']), ('DA', ['da']), ('DES', ['des']), ('UH', ['uh']), ('D', ['d']), ('DEUX', ['deux']), ('EE', ['ee']), ('DU', ['du']), ('DAY', ['day']), ('E', ['e']), ('DOUGH', ['dough']), ('DEED', ['deed']), ('DOE', ['doe']), ('DUE', ['due']), ('A', ['a']), ('DAI', ['dai']), ('DO', ['do']), ('AE', ['ae']), ('DOW', ['dow']), ('DADE', ['dade']), ('DI', ['di']), ('DEW', ['dew']), ('DER', ['der']), ('DOO', ['doo']), ('DIE', ['die']), ('TAO', ['tao']), ('DEA', ['dea']), ('DYE', ['dye'])]\n",
      "--------------------------------------------------\n",
      "{('F', 'EH', 'R'), ('F', 'ER')} ['fe', '##r']\n",
      "[('ERR', ['er', '##r']), ('PHU', ['ph', '##u']), ('ERE', ['er', '##e']), ('EURE', ['eu', '##re']), ('FERRE', ['fe', '##rre']), ('PUR', ['pu', '##r']), ('BIRR', ['bi', '##rr']), ('FIER', ['fi', '##er']), ('FEHR', ['fe', '##hr']), ('PARE', ['par', '##e']), ('FAIRE', ['fair', '##e']), ('AYRE', ['a', '##yre']), ('BUR', ['bu', '##r']), ('FURRH', ['fur', '##rh']), ('FURSE', ['fur', '##se']), ('FARED', ['fare', '##d']), ('PHAIR', ['ph', '##air']), ('BAIR', ['bai', '##r']), ('FEUR', ['fe', '##ur']), ('BEHR', ['be', '##hr']), ('BAER', ['bae', '##r']), ('AER', ['ae', '##r']), ('BIR', ['bi', '##r']), ('FAO', ['fa', '##o']), ('FURRER', ['fur', '##rer']), ('BAEHR', ['bae', '##hr']), ('FURR', ['fur', '##r']), ('BAHR', ['ba', '##hr']), ('PURR', ['pu', '##rr']), ('FERD', ['fe', '##rd'])]\n",
      "--------------------------------------------------\n",
      "{('F', 'AY'), ('F', 'IY')} ['fi']\n",
      "[('FOE', ['foe']), ('BI', ['bi']), ('B', ['b']), ('PIE', ['pie']), ('BAE', ['bae']), ('BEE', ['bee']), ('I', ['i']), ('EYE', ['eye']), ('EE', ['ee']), ('BY', ['by']), ('PHI', ['phi']), ('E', ['e']), ('BYE', ['bye']), ('P', ['p']), ('PEA', ['pea']), ('BUY', ['buy']), ('FIDE', ['fide']), ('PEE', ['pee']), ('BE', ['be']), ('FEED', ['feed']), ('PI', ['pi']), ('BEA', ['bea']), ('AI', ['ai']), ('FEE', ['fee']), ('FAE', ['fae']), ('FAUX', ['faux']), ('AYE', ['aye'])]\n",
      "--------------------------------------------------\n",
      "{('F', 'R', 'ER'), ('F', 'AO', 'R'), ('F', 'ER')} ['for']\n",
      "[('FOUR', ['four']), ('POUR', ['pour']), ('FAR', ['far']), ('ORR', ['orr']), ('UR', ['ur']), ('ARE', ['are']), ('PER', ['per']), ('POR', ['por']), ('FORD', ['ford']), ('FORE', ['fore']), ('FIR', ['fir']), ('FOO', ['foo']), ('OR', ['or']), ('BOER', ['boer']), ('PERE', ['pere']), ('ORE', ['ore']), ('BORE', ['bore']), ('FU', ['fu']), ('BOAR', ['boar']), ('FORCE', ['force']), ('BURR', ['burr']), ('ER', ['er']), ('FUR', ['fur'])]\n",
      "--------------------------------------------------\n",
      "{('F', 'ER', 'S')} ['fur', '##se']\n",
      "[('BEARSE', ['bears', '##e']), ('FOOSE', ['foo', '##se']), ('BURSE', ['bu', '##rse']), ('PERSE', ['per', '##se']), ('FUOSS', ['fu', '##oss']), ('BEARSS', ['bears', '##s']), ('PERCE', ['per', '##ce']), ('BEARCE', ['bear', '##ce']), ('FESS', ['fe', '##ss']), ('FURS', ['fur', '##s']), ('FURRH', ['fur', '##rh']), ('FEUR', ['fe', '##ur']), ('FISS', ['fis', '##s']), ('FER', ['fe', '##r']), ('FIRS', ['fir', '##s']), ('PERS', ['per', '##s']), ('FOUSE', ['f', '##ouse']), ('FURR', ['fur', '##r']), ('PEARSE', ['pear', '##se']), ('FORS', ['for', '##s']), ('FUHS', ['fu', '##hs'])]\n",
      "--------------------------------------------------\n",
      "{('HH', 'EH', 'N'), ('HH', 'EH', 'N', 'IY')} ['hen', '##ne']\n",
      "[('HERNE', ['her', '##ne']), ('HOON', ['ho', '##on']), ('HUN', ['hu', '##n']), ('N.', ['n', '.']), ('HINN', ['hi', '##nn']), ('HENIE', ['hen', '##ie']), ('HEHN', ['he', '##hn']), ('HIRN', ['hi', '##rn']), ('HEARN', ['hear', '##n']), ('HURNEY', ['hu', '##rney']), ('HERN', ['her', '##n']), ('HENN', ['hen', '##n']), ('HUHN', ['huh', '##n']), ('HENNEY', ['hen', '##ney']), ('HEH', ['he', '##h']), ('HUNN', ['hu', '##nn']), ('HURN', ['hu', '##rn']), ('HEARNE', ['hear', '##ne']), ('HENEY', ['hen', '##ey']), ('HINEY', ['hi', '##ney']), ('HENSE', ['hen', '##se'])]\n",
      "--------------------------------------------------\n",
      "{('SH', 'UW')} ['hs', '##u']\n",
      "[('CHOO', ['cho', '##o']), ('SHER', ['she', '##r']), ('OOH', ['o', '##oh']), ('SHIRR', ['shi', '##rr']), ('TUE', ['tu', '##e']), ('TEW', ['te', '##w']), ('SHOOED', ['sho', '##oed']), ('SHAO', ['sha', '##o']), ('SCHOW', ['sc', '##how']), ('SHOUGH', ['sho', '##ugh']), ('SHEW', ['she', '##w']), ('SCHER', ['sc', '##her']), ('SCHUE', ['sc', '##hue']), ('SHURR', ['shu', '##rr']), ('SHIU', ['shi', '##u']), ('CHOU', ['cho', '##u']), ('THUY', ['th', '##uy']), ('SHIR', ['shi', '##r']), ('CHIU', ['chi', '##u']), ('SCHUR', ['sc', '##hur']), ('SHUR', ['shu', '##r']), ('SHOO', ['sho', '##o']), ('SHUE', ['shu', '##e'])]\n",
      "--------------------------------------------------\n",
      "{('K', 'AA', 'R'), ('K', 'EH', 'R')} ['ka', '##hre']\n",
      "[('KIR', ['ki', '##r']), ('ERR', ['er', '##r']), ('KAHR', ['ka', '##hr']), ('KOOR', ['ko', '##or']), ('KIER', ['ki', '##er']), ('ERE', ['er', '##e']), ('KARR', ['ka', '##rr']), ('KOHR', ['ko', '##hr']), ('CARSE', ['cars', '##e']), ('COR', ['co', '##r']), ('AYRE', ['a', '##yre']), ('KEHR', ['ke', '##hr']), ('CAIRE', ['cai', '##re']), ('COEUR', ['coe', '##ur']), ('COHR', ['co', '##hr']), ('AHR', ['ah', '##r']), ('CARRE', ['carr', '##e']), ('CORR', ['co', '##rr']), ('AER', ['ae', '##r']), ('COAR', ['coa', '##r']), ('KAH', ['ka', '##h']), ('R.', ['r', '.']), ('KEAR', ['ke', '##ar']), ('COUR', ['co', '##ur'])]\n",
      "--------------------------------------------------\n",
      "{('M', 'EY', 'L', 'IY'), ('M', 'EY', 'L')} ['may', '##le']\n",
      "[('MAUL', ['ma', '##ul']), ('MOLLIE', ['mo', '##llie']), ('MALY', ['mal', '##y']), ('MAYE', ['may', '##e']), ('ALEY', ['ale', '##y']), ('AIL', ['ai', '##l']), ('MALLIE', ['mall', '##ie']), ('ALY', ['al', '##y']), ('MAHL', ['ma', '##hl']), ('MAILED', ['mail', '##ed']), ('MEY', ['me', '##y']), ('MAULE', ['ma', '##ule']), ('MOL', ['mo', '##l']), ('MAILEY', ['mail', '##ey']), ('MOLLE', ['mo', '##lle']), ('AILEY', ['ai', '##ley']), ('AILEE', ['ai', '##lee']), ('MALEY', ['male', '##y']), ('MAULL', ['ma', '##ull']), ('MAILLE', ['mail', '##le']), ('MOLL', ['mo', '##ll']), ('MOLLEY', ['mo', '##lley']), ('MAILE', ['mail', '##e'])]\n",
      "--------------------------------------------------\n",
      "{('M', 'AA', 'R', 'K'), ('M', 'ER', 'K')} ['mer', '##c']\n",
      "[('MARQUE', ['mar', '##que']), ('MURR', ['mu', '##rr']), ('MARKE', ['mark', '##e']), ('MAHR', ['ma', '##hr']), ('IRK', ['ir', '##k']), ('MAACK', ['ma', '##ack']), ('MOERKE', ['moe', '##rke']), ('MERK', ['mer', '##k']), ('MAHER', ['ma', '##her']), ('MUCK', ['mu', '##ck']), ('MURCH', ['mu', '##rch']), ('MARQUES', ['mar', '##ques']), ('MARR', ['mar', '##r']), ('MECH', ['me', '##ch']), ('MECK', ['me', '##ck']), ('ERK', ['er', '##k']), ('MOCH', ['mo', '##ch']), ('MOK', ['mo', '##k']), ('MICKE', ['mick', '##e']), ('MORK', ['mor', '##k']), ('ERCK', ['er', '##ck']), ('MURK', ['mu', '##rk']), ('MERCK', ['mer', '##ck']), ('MAWR', ['ma', '##wr']), ('MARCKS', ['marc', '##ks'])]\n",
      "--------------------------------------------------\n",
      "{('OW',), ('AW',)} ['ow']\n",
      "[('UH', ['uh']), ('O', ['o']), ('EH', ['eh']), ('I', ['i']), ('EYE', ['eye']), ('EE', ['ee']), ('AU', ['au']), ('UR', ['ur']), ('ARE', ['are']), ('OU', ['ou']), ('E', ['e']), ('AUX', ['aux']), ('OWE', ['owe']), ('OR', ['or']), ('A', ['a']), ('AI', ['ai']), ('ODE', ['ode']), ('OH', ['oh']), ('AYE', ['aye']), ('ER', ['er']), ('OWED', ['owed'])]\n",
      "--------------------------------------------------\n",
      "{('P', 'EY', 'L', 'IY')} ['pale', '##y']\n",
      "[('BALLY', ['ball', '##y']), ('BAYLY', ['bay', '##ly']), ('ALEY', ['ale', '##y']), ('PAYEE', ['pay', '##ee']), ('ALY', ['al', '##y']), ('PAULI', ['paul', '##i']), ('PAULEY', ['paul', '##ey']), ('BAILY', ['bail', '##y']), ('PULLEY', ['pull', '##ey']), ('BALEY', ['bal', '##ey']), ('AILEY', ['ai', '##ley']), ('BAILIE', ['bail', '##ie']), ('AILEE', ['ai', '##lee']), ('BAILLY', ['bail', '##ly']), ('PAULY', ['paul', '##y']), ('POLLEY', ['poll', '##ey']), ('BAILLIE', ['bail', '##lie']), ('BAYLEE', ['bay', '##lee']), ('PAULIE', ['paul', '##ie']), ('FALEY', ['fa', '##ley']), ('BAYLEY', ['bay', '##ley']), ('PAWLEY', ['paw', '##ley'])]\n",
      "--------------------------------------------------\n",
      "{('P', 'ER')} ['pu', '##r']\n",
      "[('ERR', ['er', '##r']), ('POUGH', ['po', '##ugh']), ('PERSE', ['per', '##se']), ('EURE', ['eu', '##re']), ('BIRR', ['bi', '##rr']), ('POO', ['po', '##o']), ('PERCE', ['per', '##ce']), ('BUR', ['bu', '##r']), ('FURRH', ['fur', '##rh']), ('POOH', ['po', '##oh']), ('FEUR', ['fe', '##ur']), ('PAO', ['pa', '##o']), ('POWE', ['pow', '##e']), ('PIH', ['pi', '##h']), ('FER', ['fe', '##r']), ('BIR', ['bi', '##r']), ('PERS', ['per', '##s']), ('FURR', ['fur', '##r']), ('POU', ['po', '##u']), ('PURR', ['pu', '##rr']), ('PEARSE', ['pear', '##se'])]\n",
      "--------------------------------------------------\n",
      "{('P', 'ER')} ['pu', '##rr']\n",
      "[('ERR', ['er', '##r']), ('POUGH', ['po', '##ugh']), ('PERSE', ['per', '##se']), ('EURE', ['eu', '##re']), ('PUR', ['pu', '##r']), ('BIRR', ['bi', '##rr']), ('POO', ['po', '##o']), ('PERCE', ['per', '##ce']), ('BUR', ['bu', '##r']), ('FURRH', ['fur', '##rh']), ('POOH', ['po', '##oh']), ('FEUR', ['fe', '##ur']), ('PAO', ['pa', '##o']), ('POWE', ['pow', '##e']), ('PIH', ['pi', '##h']), ('FER', ['fe', '##r']), ('BIR', ['bi', '##r']), ('PERS', ['per', '##s']), ('FURR', ['fur', '##r']), ('POU', ['po', '##u']), ('PEARSE', ['pear', '##se'])]\n",
      "--------------------------------------------------\n",
      "{('K', 'EY'), ('K', 'IY')} ['quay']\n",
      "[('KAY', ['kay']), ('CASE', ['case']), ('CADE', ['cade']), ('KAI', ['kai']), ('EE', ['ee']), ('KEY', ['key']), ('KADE', ['kade']), ('E', ['e']), ('KA', ['ka']), ('CA', ['ca']), ('QI', ['qi']), ('CO', ['co']), ('A', ['a']), ('QUI', ['qui']), ('COE', ['coe']), ('AE', ['ae']), ('KI', ['ki']), ('KO', ['ko']), ('CHI', ['chi']), ('KAYE', ['kaye']), ('CAI', ['cai']), ('K', ['k'])]\n",
      "--------------------------------------------------\n",
      "{('R', 'IY'), ('R', 'EY')} ['re']\n",
      "[('REID', ['reid']), ('RYE', ['rye']), ('RAY', ['ray']), ('RA', ['ra']), ('REED', ['reed']), ('EE', ['ee']), ('WRY', ['wry']), ('READ', ['read']), ('E', ['e']), ('RAE', ['rae']), ('RO', ['ro']), ('REY', ['rey']), ('RHYS', ['rhys']), ('ROW', ['row']), ('REESE', ['reese']), ('REECE', ['reece']), ('A', ['a']), ('ROWE', ['rowe']), ('RAW', ['raw']), ('ROE', ['roe']), ('AE', ['ae']), ('RAID', ['raid']), ('RACE', ['race'])]\n",
      "--------------------------------------------------\n",
      "{('SH', 'UW')} ['sc', '##hue']\n",
      "[('CHOO', ['cho', '##o']), ('SHER', ['she', '##r']), ('OOH', ['o', '##oh']), ('SHIRR', ['shi', '##rr']), ('TUE', ['tu', '##e']), ('TEW', ['te', '##w']), ('SHOOED', ['sho', '##oed']), ('SHAO', ['sha', '##o']), ('SCHOW', ['sc', '##how']), ('SHOUGH', ['sho', '##ugh']), ('SHEW', ['she', '##w']), ('SCHER', ['sc', '##her']), ('SHURR', ['shu', '##rr']), ('SHIU', ['shi', '##u']), ('CHOU', ['cho', '##u']), ('THUY', ['th', '##uy']), ('SHIR', ['shi', '##r']), ('CHIU', ['chi', '##u']), ('SCHUR', ['sc', '##hur']), ('SHUR', ['shu', '##r']), ('SHOO', ['sho', '##o']), ('HSU', ['hs', '##u']), ('SHUE', ['shu', '##e'])]\n",
      "--------------------------------------------------\n",
      "{('SH', 'UW')} ['she', '##w']\n",
      "[('CHOO', ['cho', '##o']), ('SHER', ['she', '##r']), ('OOH', ['o', '##oh']), ('SHIRR', ['shi', '##rr']), ('TUE', ['tu', '##e']), ('TEW', ['te', '##w']), ('SHOOED', ['sho', '##oed']), ('SHAO', ['sha', '##o']), ('SCHOW', ['sc', '##how']), ('SHOUGH', ['sho', '##ugh']), ('SCHER', ['sc', '##her']), ('SCHUE', ['sc', '##hue']), ('SHURR', ['shu', '##rr']), ('SHIU', ['shi', '##u']), ('CHOU', ['cho', '##u']), ('THUY', ['th', '##uy']), ('SHIR', ['shi', '##r']), ('CHIU', ['chi', '##u']), ('SCHUR', ['sc', '##hur']), ('SHUR', ['shu', '##r']), ('SHOO', ['sho', '##o']), ('HSU', ['hs', '##u']), ('SHUE', ['shu', '##e'])]\n",
      "--------------------------------------------------\n",
      "{('SH', 'UW')} ['shi', '##u']\n",
      "[('CHOO', ['cho', '##o']), ('SHER', ['she', '##r']), ('OOH', ['o', '##oh']), ('SHIRR', ['shi', '##rr']), ('TUE', ['tu', '##e']), ('TEW', ['te', '##w']), ('SHOOED', ['sho', '##oed']), ('SHAO', ['sha', '##o']), ('SCHOW', ['sc', '##how']), ('SHOUGH', ['sho', '##ugh']), ('SHEW', ['she', '##w']), ('SCHER', ['sc', '##her']), ('SCHUE', ['sc', '##hue']), ('SHURR', ['shu', '##rr']), ('CHOU', ['cho', '##u']), ('THUY', ['th', '##uy']), ('SHIR', ['shi', '##r']), ('CHIU', ['chi', '##u']), ('SCHUR', ['sc', '##hur']), ('SHUR', ['shu', '##r']), ('SHOO', ['sho', '##o']), ('HSU', ['hs', '##u']), ('SHUE', ['shu', '##e'])]\n",
      "--------------------------------------------------\n",
      "{('SH', 'UW')} ['sho', '##o']\n",
      "[('CHOO', ['cho', '##o']), ('SHER', ['she', '##r']), ('OOH', ['o', '##oh']), ('SHIRR', ['shi', '##rr']), ('TUE', ['tu', '##e']), ('TEW', ['te', '##w']), ('SHOOED', ['sho', '##oed']), ('SHAO', ['sha', '##o']), ('SCHOW', ['sc', '##how']), ('SHOUGH', ['sho', '##ugh']), ('SHEW', ['she', '##w']), ('SCHER', ['sc', '##her']), ('SCHUE', ['sc', '##hue']), ('SHURR', ['shu', '##rr']), ('SHIU', ['shi', '##u']), ('CHOU', ['cho', '##u']), ('THUY', ['th', '##uy']), ('SHIR', ['shi', '##r']), ('CHIU', ['chi', '##u']), ('SCHUR', ['sc', '##hur']), ('SHUR', ['shu', '##r']), ('HSU', ['hs', '##u']), ('SHUE', ['shu', '##e'])]\n",
      "--------------------------------------------------\n",
      "{('SH', 'UW')} ['shu', '##e']\n",
      "[('CHOO', ['cho', '##o']), ('SHER', ['she', '##r']), ('OOH', ['o', '##oh']), ('SHIRR', ['shi', '##rr']), ('TUE', ['tu', '##e']), ('TEW', ['te', '##w']), ('SHOOED', ['sho', '##oed']), ('SHAO', ['sha', '##o']), ('SCHOW', ['sc', '##how']), ('SHOUGH', ['sho', '##ugh']), ('SHEW', ['she', '##w']), ('SCHER', ['sc', '##her']), ('SCHUE', ['sc', '##hue']), ('SHURR', ['shu', '##rr']), ('SHIU', ['shi', '##u']), ('CHOU', ['cho', '##u']), ('THUY', ['th', '##uy']), ('SHIR', ['shi', '##r']), ('CHIU', ['chi', '##u']), ('SCHUR', ['sc', '##hur']), ('SHUR', ['shu', '##r']), ('SHOO', ['sho', '##o']), ('HSU', ['hs', '##u'])]\n",
      "--------------------------------------------------\n",
      "{('S', 'AW'), ('S', 'OW')} ['so', '##w']\n",
      "[('SIEW', ['si', '##ew']), ('SIEH', ['si', '##eh']), ('CIE', ['ci', '##e']), ('OHH', ['oh', '##h']), ('AUE', ['au', '##e']), (\"O'\", ['o', \"'\"]), ('SUU', ['su', '##u']), ('O.', ['o', '.']), ('SUH', ['su', '##h']), ('SIAS', ['si', '##as']), ('TSAI', ['ts', '##ai']), ('TSO', ['ts', '##o']), ('TSE', ['ts', '##e']), ('SIE', ['si', '##e']), ('ZOH', ['z', '##oh']), ('SOWED', ['so', '##wed']), ('SEW', ['se', '##w']), ('C.', ['c', '.']), ('SEWED', ['se', '##wed']), ('EAUX', ['ea', '##ux']), ('TSAO', ['ts', '##ao']), ('EAU', ['ea', '##u'])]\n",
      "--------------------------------------------------\n",
      "{('T', 'AY'), ('T', 'EY')} ['tae']\n",
      "[('TOE', ['toe']), ('TE', ['te']), ('TI', ['ti']), ('SHEA', ['shea']), ('I', ['i']), ('TOW', ['tow']), ('TEA', ['tea']), ('THAI', ['thai']), ('EYE', ['eye']), ('THY', ['thy']), ('TAI', ['tai']), ('TY', ['ty']), ('THEY', ['they']), ('T', ['t']), ('TIED', ['tied']), ('A', ['a']), ('AI', ['ai']), ('TAY', ['tay']), ('TIDE', ['tide']), ('TIE', ['tie']), ('AE', ['ae']), ('SHAY', ['shay']), ('SHY', ['shy']), ('TA', ['ta']), ('TEE', ['tee']), ('AYE', ['aye']), ('CHE', ['che'])]\n",
      "--------------------------------------------------\n",
      "{('T', 'AW'), ('D', 'AW')} ['tao']\n",
      "[('DES', ['des']), ('TWO', ['two']), ('DEUX', ['deux']), ('TER', ['ter']), ('XIAO', ['xiao']), ('DU', ['du']), ('TO', ['to']), ('DE', ['de']), ('CHAO', ['chao']), ('TOO', ['too']), ('DUE', ['due']), ('TU', ['tu']), ('THOU', ['thou']), ('DO', ['do']), ('DOW', ['dow']), ('DEW', ['dew']), ('DER', ['der']), ('ZHAO', ['zhao']), ('CHOW', ['chow']), ('DOO', ['doo']), ('OW', ['ow'])]\n",
      "--------------------------------------------------\n",
      "{('T', 'OW')} ['toe']\n",
      "[('TE', ['te']), ('O', ['o']), ('TAE', ['tae']), ('TI', ['ti']), ('THOUGH', ['though']), ('TEA', ['tea']), ('TOW', ['tow']), ('THAI', ['thai']), ('AU', ['au']), ('AUX', ['aux']), ('TAI', ['tai']), ('OWE', ['owe']), ('TY', ['ty']), ('THO', ['tho']), ('T', ['t']), ('TOWED', ['towed']), ('TIE', ['tie']), ('TOAD', ['toad']), ('TEE', ['tee']), ('OH', ['oh']), ('CHO', ['cho']), ('SHOW', ['show']), ('OW', ['ow'])]\n",
      "--------------------------------------------------\n",
      "{('T', 'OW')} ['tow']\n",
      "[('TOE', ['toe']), ('TE', ['te']), ('O', ['o']), ('TAE', ['tae']), ('TI', ['ti']), ('THOUGH', ['though']), ('TEA', ['tea']), ('THAI', ['thai']), ('AU', ['au']), ('AUX', ['aux']), ('TAI', ['tai']), ('OWE', ['owe']), ('TY', ['ty']), ('THO', ['tho']), ('T', ['t']), ('TOWED', ['towed']), ('TIE', ['tie']), ('TOAD', ['toad']), ('TEE', ['tee']), ('OH', ['oh']), ('CHO', ['cho']), ('SHOW', ['show']), ('OW', ['ow'])]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for w in idx2word:\n",
    "    if len(word_confusions[w]) > 20:\n",
    "        print(word2pron[w], word2pieces[w])\n",
    "        print([(cw, word2pieces[cw]) for cw in word_confusions[w]])\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({('TH', 'R', 'IY')}, ['three'])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 'THREE'\n",
    "word2pron[w], word2pieces[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('THROW', ['throw'], {('TH', 'R', 'OW')}),\n",
       " ('RE', ['re'], {('R', 'EY'), ('R', 'IY')})]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(cw, word2pieces[cw], word2pron[cw]) for cw in word_confusions[w]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# GPT2 lm score \n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model_lm = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = gpt2_tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = gpt2_model_lm(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss, logits = outputs[:2]\n",
    "loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT2_LM_Loss(sentence: str, gpt2_tokenizer: GPT2Tokenizer, gpt2_model_lm: GPT2LMHeadModel) -> float:\n",
    "    inputs = gpt2_tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = gpt2_model_lm(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs[0].item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.128024578094482"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT2_LM_Loss(\"Hello, my dog is very cute .\", gpt2_tokenizer, gpt2_model_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAcousticConfuser(object):\n",
    "    def __init__(self,\n",
    "                 word_confusions_path: str):\n",
    "                 # fix_subword_lengths: bool = True,  # confusion word have same number of subwords as original word \n",
    "                 # bert_tokenizer_name: str = 'bert-base-uncased'\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_confusions_path = word_confusions_path\n",
    "        # self.fix_subword_lengths = fix_subword_lengths\n",
    "        # self.bert_tokenizer_name = bert_tokenizer_name\n",
    "        \n",
    "        with open(self.word_confusions_path, 'rb') as f:\n",
    "            self.word_confusions = pickle.load(f)\n",
    "    \n",
    "    def sentence_confuse(self, sentence: List[str], p: float) -> List[str]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAcousticConfuser_RandomReplace(SentenceAcousticConfuser):\n",
    "    def __init__(self,\n",
    "                 word_confusions_path: str):\n",
    "        \n",
    "        super().__init__(word_confusions_path)\n",
    "    \n",
    "    def sentence_confuse(self, sentence: List[str], p: float) -> List[str]:\n",
    "        assert 0 <= p <= 1\n",
    "    \n",
    "        sen_len = len(sentence)\n",
    "        confs_cnt = int(p * sen_len)\n",
    "\n",
    "        confusable_positions = []\n",
    "        for pos in range(sen_len):\n",
    "            word = sentence[pos].upper()\n",
    "            if len(self.word_confusions[word]) > 0:\n",
    "                confusable_positions.append(pos)\n",
    "\n",
    "        if len(confusable_positions) <= confs_cnt:\n",
    "            # not enough positions for confusion\n",
    "            confs_positions = confusable_positions\n",
    "        else:\n",
    "            confs_positions = random.sample(confusable_positions, k=confs_cnt)\n",
    "\n",
    "        confs_sentence = list(sentence)\n",
    "        for pos in confs_positions:\n",
    "            word = sentence[pos].upper()\n",
    "            confs_word = random.choice(list(self.word_confusions[word])).lower()\n",
    "            if pos == 0:\n",
    "                confs_word = confs_word.capitalize()\n",
    "            confs_sentence[pos] = confs_word\n",
    "\n",
    "        return confs_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAcousticConfuser_GPT2LossReplace(SentenceAcousticConfuser):\n",
    "    def __init__(self,\n",
    "                 word_confusions_path: str):\n",
    "        \n",
    "        super().__init__(word_confusions_path)\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.gpt2_model_lm = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    def _gpt2_lm_loss(self, sentence: List[str]) -> float:\n",
    "        inputs = self.gpt2_tokenizer(' '.join(sentence), return_tensors=\"pt\")\n",
    "        outputs = self.gpt2_model_lm(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs[0].item()\n",
    "        return loss\n",
    "    \n",
    "    def sentence_confuse(self, sentence: List[str], p: float) -> List[str]:\n",
    "        assert 0 <= p <= 1\n",
    "    \n",
    "        sen_len = len(sentence)\n",
    "        confs_cnt = int(p * sen_len)\n",
    "\n",
    "        confusable_positions = []\n",
    "        for pos in range(sen_len):\n",
    "            word = sentence[pos].upper()\n",
    "            if len(word_confusions[word]) > 0:\n",
    "                confusable_positions.append(pos)\n",
    "\n",
    "        if len(confusable_positions) <= confs_cnt:\n",
    "            # not enough positions for confusion\n",
    "            confs_positions = confusable_positions\n",
    "        else:\n",
    "            confs_positions = sorted(random.sample(confusable_positions, k=confs_cnt))\n",
    "\n",
    "        confs_sentence = list(sentence)\n",
    "        for pos in confs_positions:\n",
    "            word = sentence[pos].upper()\n",
    "            assert len(self.word_confusions[word]) > 0\n",
    "\n",
    "            if len(self.word_confusions[word]) == 1:\n",
    "                # No need for LM loss \n",
    "                _cw = next(iter(self.word_confusions[word])).lower()\n",
    "                if pos == 0:\n",
    "                    _cw = _cw.capitalize()\n",
    "                confs_sentence[pos] = _cw\n",
    "                continue\n",
    "\n",
    "            # Compare different confusion words by their LM losses \n",
    "            best_lm_loss = np.inf\n",
    "            best_confs_word = None\n",
    "            for _confs_word in self.word_confusions[word]:\n",
    "                _cw = _confs_word.lower()\n",
    "                if pos == 0:\n",
    "                    _cw = _cw.capitalize()\n",
    "\n",
    "                _confs_sen = list(confs_sentence)\n",
    "                _confs_sen[pos] = _cw\n",
    "\n",
    "                _loss = self._gpt2_lm_loss(_confs_sen)\n",
    "                # print(_confs_sen, _loss)\n",
    "\n",
    "                if _loss < best_lm_loss:\n",
    "                    best_lm_loss = _loss\n",
    "                    best_confs_word = _cw\n",
    "\n",
    "            assert best_confs_word is not None\n",
    "            confs_sentence[pos] = best_confs_word\n",
    "\n",
    "        return confs_sentence\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "word_confusions_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/TaBERT/data/word_confusions.pkl'\n",
    "\n",
    "confuser_random = SentenceAcousticConfuser_RandomReplace(word_confusions_path)\n",
    "confuser_gpt2 = SentenceAcousticConfuser_GPT2LossReplace(word_confusions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.SentenceAcousticConfuser_RandomReplace at 0x147ea3f50>,\n",
       " <__main__.SentenceAcousticConfuser_GPT2LossReplace at 0x1a0d841d0>)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confuser_random, confuser_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'are', 'the', 'ids', 'of', 'the', 'TV', 'channels', 'that', 'do', 'na', 'have', 'any', 'cartoons', 'erected', 'by', 'Ben', 'Jones', '?']\n",
      "['What', 'are', 'the', 'ids', 'a', 'to', 'TV', 'channels', 'that', 'do', 'not', 'have', 'any', 'cartoons', 'directed', 'by', 'Ben', 'Jones', '?']\n"
     ]
    }
   ],
   "source": [
    "sentence = [\n",
    "    \"What\",\n",
    "    \"are\",\n",
    "    \"the\",\n",
    "    \"ids\",\n",
    "    \"of\",\n",
    "    \"the\",\n",
    "    \"TV\",\n",
    "    \"channels\",\n",
    "    \"that\",\n",
    "    \"do\",\n",
    "    \"not\",\n",
    "    \"have\",\n",
    "    \"any\",\n",
    "    \"cartoons\",\n",
    "    \"directed\",\n",
    "    \"by\",\n",
    "    \"Ben\",\n",
    "    \"Jones\",\n",
    "    \"?\"\n",
    "]\n",
    "\n",
    "print(confuser_random.sentence_confuse(sentence, p=0.15))\n",
    "print(confuser_gpt2.sentence_confuse(sentence, p=0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (trial) Apply on tables.jsonl \n",
    "tabert_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/TaBERT_datasets/tables_sample.jsonl'\n",
    "\n",
    "with open(tabert_dataset_path, 'r') as f:\n",
    "    l = f.readline()\n",
    "\n",
    "# print(ujson.dumps(ujson.loads(l), indent=4))\n",
    "d = ujson.loads(l)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = 0.15\n",
    "\n",
    "confs_d = d.copy()\n",
    "for idx, sen in enumerate(d['context_before']):\n",
    "    sen_tokens = word_tokenize(sen)\n",
    "    sen_tokens_confused = SentenceAcousticConfusion(sen_tokens, p)\n",
    "    sen_confused = ' '.join(sen_tokens_confused)\n",
    "    confs_d['context_before'][idx] = sen_confused\n",
    "for idx, sen in enumerate(d['context_after']):\n",
    "    sen_tokens = word_tokenize(sen)\n",
    "    sen_tokens_confused = SentenceAcousticConfusion(sen_tokens, p)\n",
    "    sen_confused = ' '.join(sen_tokens_confused)\n",
    "    confs_d['context_after'][idx] = sen_confused\n",
    "confs_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check preprocessed sample data \n",
    "sample_dir = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/TaBERT_datasets/train_data/vanilla_tabert_sample_ac3/train'\n",
    "sample_json_path = os.path.join(sample_dir, 'epoch_0.shard0.sample.json')\n",
    "sample_h5_path = os.path.join(sample_dir, 'epoch_0.shard0.h5')\n",
    "\n",
    "with open(sample_json_path, 'r') as f:\n",
    "    sample_json = [ujson.loads(l) for l in f]\n",
    "\n",
    "with h5py.File(sample_h5_path, 'r') as f:\n",
    "    sample_h5 = {k : np.array(v) for k, v in f.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74,\n",
       " ['tokens',\n",
       "  'token_ids',\n",
       "  'segment_a_length',\n",
       "  'masked_lm_positions',\n",
       "  'masked_lm_labels',\n",
       "  'masked_lm_label_ids',\n",
       "  'info',\n",
       "  'tokens_ref',\n",
       "  'token_ref_ids',\n",
       "  'source'])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_json), list(sample_json[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['masked_lm_label_ids',\n",
       " 'masked_lm_offsets',\n",
       " 'masked_lm_positions',\n",
       " 'segment_a_lengths',\n",
       " 'sequence_offsets',\n",
       " 'sequences',\n",
       " 'sequences_ref']"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sample_h5.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168859,)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_h5['sequences'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1424\n"
     ]
    }
   ],
   "source": [
    "l0 = list(sample_json[0]['token_ids'])\n",
    "l = list(sample_h5['sequences'])\n",
    "\n",
    "for _idx in range(len(l) - len(l0)):\n",
    "    if l[_idx : _idx + len(l0)] == l0:\n",
    "        print(_idx)\n",
    "        idx = _idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 103, 2017, 17816, 1037, 7615, 2008, 2017, 3373, 23640, 2015, 2151, 1997, 1996, 11594, 14801, 2682, 1010, 3531, 11562, 1000, 5210, 15501, 15884, 1012, 103, 4067, 2017, 1012, 7929, 102, 103, 1064, 13319, 1064, 1014, 8466, 102, 2580, 1064, 2613, 1064, 2184, 2086, 1023, 3134, 3283, 102, 2197, 7514, 1064, 2613, 1064, 1050, 1013, 1037, 102]\n",
      "[101, 103, 2017, 17816, 1037, 7615, 2008, 2017, 3373, 23640, 2015, 2151, 1997, 1996, 11594, 14801, 2682, 1010, 3531, 11562, 1000, 5210, 15501, 15884, 1012, 103, 4067, 2017, 1012, 7929, 102, 103, 1064, 13319, 1064, 1014, 8466, 102, 2580, 1064, 2613, 1064, 2184, 2086, 1023, 3134, 3283, 102, 2197, 7514, 1064, 2613, 1064, 1050, 1013, 1037, 102]\n"
     ]
    }
   ],
   "source": [
    "print(l[idx : idx + len(l0)])\n",
    "print(l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 103, 2017, 2156, 1037, 7615, 2008, 2017, 2903, 23640, 2015, 2151, 1997, 1996, 11594, 14801, 2682, 1010, 3531, 11562, 1000, 5210, 2004, 15884, 1012, 103, 4067, 2017, 1012, 7929, 102, 103, 1064, 13319, 1064, 1014, 8466, 102, 2580, 1064, 2613, 1064, 2184, 2086, 1023, 3134, 3283, 102, 2197, 7514, 1064, 2613, 1064, 1050, 1013, 1037, 102]\n",
      "[101, 103, 2017, 2156, 1037, 7615, 2008, 2017, 2903, 23640, 2015, 2151, 1997, 1996, 11594, 14801, 2682, 1010, 3531, 11562, 1000, 5210, 2004, 15884, 1012, 103, 4067, 2017, 1012, 7929, 102, 103, 1064, 13319, 1064, 1014, 8466, 102, 2580, 1064, 2613, 1064, 2184, 2086, 1023, 3134, 3283, 102, 2197, 7514, 1064, 2613, 1064, 1050, 1013, 1037, 102]\n"
     ]
    }
   ],
   "source": [
    "print(list(sample_json[0]['token_ref_ids']))\n",
    "print(list(sample_h5['sequences_ref'])[idx : idx + len(l0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n",
      "[2065, 2156, 2903, 2004, 15884, 1000, 14054, 2613]\n",
      "[2065, 2156, 2903, 2004, 15884, 1000, 14054, 2613]\n"
     ]
    }
   ],
   "source": [
    "l0 = list(sample_json[0]['masked_lm_label_ids'])\n",
    "l = list(sample_h5['masked_lm_label_ids'])\n",
    "\n",
    "for _idx in range(len(l) - len(l0)):\n",
    "    if l[_idx : _idx + len(l0)] == l0:\n",
    "        print(_idx)\n",
    "        idx = _idx\n",
    "\n",
    "print(l[idx : idx + len(l0)])\n",
    "print(l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 8, 22, 23, 25, 31, 33]\n",
      "[1, 3, 8, 22, 23, 25, 31, 33]\n"
     ]
    }
   ],
   "source": [
    "print(list(sample_json[0]['masked_lm_positions']))\n",
    "print(list(sample_h5['masked_lm_positions'])[idx : idx + len(l0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'a', 'mr', '.', \"'\", 'h', '##yu', '##sy', '##ahu', '##ti', \"'\", ',', 'with', 'value', '10', '.', '0', 'to', 'test', 'token', '##izer', '##x', '##s', '.', 'don', \"'\", 't', 'mind', 'if', 'you', \"'\", 've', 'you', \"'\", 're', '!']\n"
     ]
    }
   ],
   "source": [
    "_sentence = \"I'm a Mr. 'hyusyahuti', with value 10.0 to test tokenizerxs. Don't mind if you've you're!\"\n",
    "_sentence_tok = bert_tokenizer.tokenize(_sentence)\n",
    "print(_sentence_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images', 'reginald', 'barclay', 'reginald', 'barclay', 'the', 'next', 'generation', 'title', 'role', 'disc', 'no', 'episode', 'number', 'air', '##date', 'star', '##date', 'season', 'year', 'rating', 'hollow', 'pursuits', 'lieutenant', 'reginald', 'barclay', '3', '.', '5', '68', '30', 'apr', '1990', '43', '##80', '##7', '.', '4', '3', '236', '##6', 'the', 'n', \"'\", 'th', 'degree', 'lieutenant', 'reginald', 'barclay', '4', '.', '4', '92', '1', 'apr', '1991', '44', '##70', '##4', '.', '2', '4', '236', '##7', 'realm', 'of', 'fear', 'lieutenant', 'reginald', 'barclay', '6', '.', '1', '127', '28', 'sep', '1992', '460', '##41', '.', '1', '6', '236', '##9', 'ship', 'in', 'a', 'bottle', 'lieutenant', 'reginald', 'barclay', '6', '.', '3', '137', '25', 'jan', '1993', '46', '##42', '##4', '.', '1', '6', '236', '##9'] 106\n"
     ]
    }
   ],
   "source": [
    "_sentence_tok = ['images', 'reginald', 'barclay', 'reginald', 'barclay', 'the', 'next', 'generation', 'title', 'role', 'disc', 'no', 'episode', 'number', 'air', '##date', 'star', '##date', 'season', 'year', 'rating', 'hollow', 'pursuits', 'lieutenant', 'reginald', 'barclay', '3', '.', '5', '68', '30', 'apr', '1990', '43', '##80', '##7', '.', '4', '3', '236', '##6', 'the', 'n', \"'\", 'th', 'degree', 'lieutenant', 'reginald', 'barclay', '4', '.', '4', '92', '1', 'apr', '1991', '44', '##70', '##4', '.', '2', '4', '236', '##7', 'realm', 'of', 'fear', 'lieutenant', 'reginald', 'barclay', '6', '.', '1', '127', '28', 'sep', '1992', '460', '##41', '.', '1', '6', '236', '##9', 'ship', 'in', 'a', 'bottle', 'lieutenant', 'reginald', 'barclay', '6', '.', '3', '137', '25', 'jan', '1993', '46', '##42', '##4', '.', '1', '6', '236', '##9']\n",
    "print(_sentence_tok, len(_sentence_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_BertTokenizer(sentence: List[str]) -> str:\n",
    "    return (\n",
    "        \" \".join(sentence)\n",
    "        .replace(\" ##\", \"\")\n",
    "#         .replace(\" .\", \".\")\n",
    "#         .replace(\" ?\", \"?\")\n",
    "#         .replace(\" !\", \"!\")\n",
    "#         .replace(\" ,\", \",\")\n",
    "#         .replace(\" ' \", \"'\")\n",
    "#         .replace(\" n't\", \"n't\")\n",
    "#         .replace(\" 'm\", \"'m\")\n",
    "#         .replace(\" 's\", \"'s\")\n",
    "#         .replace(\" 've\", \"'ve\")\n",
    "#         .replace(\" 're\", \"'re\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"images reginald barclay reginald barclay the next generation title role disc no episode number airdate stardate season year rating hollow pursuits lieutenant reginald barclay 3 . 5 68 30 apr 1990 43807 . 4 3 2366 the n ' th degree lieutenant reginald barclay 4 . 4 92 1 apr 1991 44704 . 2 4 2367 realm of fear lieutenant reginald barclay 6 . 1 127 28 sep 1992 46041 . 1 6 2369 ship in a bottle lieutenant reginald barclay 6 . 3 137 25 jan 1993 46424 . 1 6 2369\""
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sentence_detok = detokenize_BertTokenizer(_sentence_tok)\n",
    "_sentence_detok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images', 'reginald', 'barclay', 'reginald', 'barclay', 'the', 'next', 'generation', 'title', 'role', 'disc', 'no', 'episode', 'number', 'air', '##date', 'star', '##date', 'season', 'year', 'rating', 'hollow', 'pursuits', 'lieutenant', 'reginald', 'barclay', '3', '.', '5', '68', '30', 'apr', '1990', '43', '##80', '##7', '.', '4', '3', '236', '##6', 'the', 'n', \"'\", 'th', 'degree', 'lieutenant', 'reginald', 'barclay', '4', '.', '4', '92', '1', 'apr', '1991', '44', '##70', '##4', '.', '2', '4', '236', '##7', 'realm', 'of', 'fear', 'lieutenant', 'reginald', 'barclay', '6', '.', '1', '127', '28', 'sep', '1992', '460', '##41', '.', '1', '6', '236', '##9', 'ship', 'in', 'a', 'bottle', 'lieutenant', 'reginald', 'barclay', '6', '.', '3', '137', '25', 'jan', '1993', '46', '##42', '##4', '.', '1', '6', '236', '##9']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.tokenize(_sentence_detok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize(_sentence_detok) == _sentence_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
